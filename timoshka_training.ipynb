{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training custom wake word \"Тимошка\" for openWakeWord\n\nFull pipeline: Russian Piper TTS → Voice Conversion (FreeVC24) → openWakeWord Training → TFLite\n\n**Requirements:** Google Colab Pro with A100 (recommended). Runtime: 3-6 hours.\n\n## Stages\n1. Install dependencies\n2. Generate TTS samples (Piper, Russian voices)\n3. Prepare target voices from Common Voice\n4. Voice conversion (FreeVC24)\n5. Download negative data (ACAV100M)\n6. Train openWakeWord\n7. Convert to TFLite\n8. Test the model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repositories\n",
    "cd /content\n",
    "\n",
    "if [ ! -d \"openWakeWord\" ]; then\n",
    "    git clone https://github.com/dscripka/openWakeWord.git\n",
    "fi\n",
    "\n",
    "if [ ! -d \"piper-sample-generator\" ]; then\n",
    "    git clone https://github.com/rhasspy/piper-sample-generator.git\n",
    "fi\n",
    "\n",
    "echo \"Done cloning repos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\n# Install dependencies\n# piper-phonemize needs wheels from GitHub releases\npip install -q piper-phonemize -f https://github.com/rhasspy/piper-phonemize/releases/latest\npip install -q webrtcvad\npip install -q -e /content/openWakeWord\npip install -q -e /content/piper-sample-generator\npip install -q coqui-tts\npip install -q mutagen\npip install -q torchinfo\npip install -q torchmetrics\npip install -q speechbrain\npip install -q audiomentations\npip install -q torch-audiomentations\npip install -q acoustics\npip install -q pronouncing\npip install -q deep-phonemizer\n# Use Colab's pre-installed TensorFlow, just add onnx conversion\npip install -q onnx onnx2tf tf2onnx flatbuffers\n\necho \"\\nAll dependencies installed\"\necho \"IMPORTANT: If this is a fresh runtime, restart it now (Runtime -> Restart session)\"\necho \"Then re-run all cells. Files on disk will be preserved.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Working directories\n",
    "BASE_DIR = \"/content/timoshka\"\n",
    "VOICES_DIR = os.path.join(BASE_DIR, \"piper_voices\")\n",
    "TTS_POSITIVE_DIR = os.path.join(BASE_DIR, \"tts_positive\")\n",
    "TTS_NEGATIVE_DIR = os.path.join(BASE_DIR, \"tts_negative\")\n",
    "VOICE_TARGETS_DIR = os.path.join(BASE_DIR, \"voice_targets\")\n",
    "VC_POSITIVE_DIR = os.path.join(BASE_DIR, \"vc_positive\")\n",
    "VC_NEGATIVE_DIR = os.path.join(BASE_DIR, \"vc_negative\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "\n",
    "for d in [\n",
    "    BASE_DIR, VOICES_DIR, TTS_POSITIVE_DIR, TTS_NEGATIVE_DIR,\n",
    "    VOICE_TARGETS_DIR, VC_POSITIVE_DIR, VC_NEGATIVE_DIR, OUTPUT_DIR,\n",
    "]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Generate TTS samples (Piper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Russian Piper voices\n",
    "cd /content/timoshka/piper_voices\n",
    "\n",
    "VOICES=(\n",
    "    \"ru/ru_RU/irina/medium/ru_RU-irina-medium\"\n",
    "    \"ru/ru_RU/ruslan/medium/ru_RU-ruslan-medium\"\n",
    "    \"ru/ru_RU/denis/medium/ru_RU-denis-medium\"\n",
    "    \"ru/ru_RU/dmitri/medium/ru_RU-dmitri-medium\"\n",
    ")\n",
    "\n",
    "BASE_URL=\"https://huggingface.co/rhasspy/piper-voices/resolve/main\"\n",
    "\n",
    "for voice in \"${VOICES[@]}\"; do\n",
    "    name=$(basename $voice)\n",
    "    if [ ! -f \"${name}.onnx\" ]; then\n",
    "        echo \"Downloading ${name}...\"\n",
    "        wget -q -O \"${name}.onnx\" \"${BASE_URL}/${voice}.onnx?download=true\"\n",
    "        wget -q -O \"${name}.onnx.json\" \"${BASE_URL}/${voice}.onnx.json?download=true\"\n",
    "    else\n",
    "        echo \"${name} already downloaded\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"\\nAll voices downloaded:\"\n",
    "ls -la *.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "\n",
    "# Positive samples: 250 per voice x 4 voices = 1000 base samples\n",
    "\n",
    "voices = sorted(glob.glob(os.path.join(VOICES_DIR, \"*.onnx\")))\n",
    "print(f\"Found {len(voices)} Piper voices\")\n",
    "\n",
    "POSITIVE_PHRASE = \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"  # тимошка\n",
    "SAMPLES_PER_VOICE = 250\n",
    "\n",
    "for voice_path in voices:\n",
    "    voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n",
    "    out_dir = os.path.join(TTS_POSITIVE_DIR, voice_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "    if existing >= SAMPLES_PER_VOICE:\n",
    "        print(f\"  {voice_name}: {existing} samples already exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Generating {SAMPLES_PER_VOICE} positive samples with {voice_name}...\")\n",
    "    subprocess.run([\n",
    "        \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n",
    "        POSITIVE_PHRASE,\n",
    "        \"--model\", voice_path,\n",
    "        \"--max-samples\", str(SAMPLES_PER_VOICE),\n",
    "        \"--output-dir\", out_dir,\n",
    "    ], check=True)\n",
    "\n",
    "    generated = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "    print(f\"    Generated: {generated}\")\n",
    "\n",
    "total = sum(\n",
    "    len(glob.glob(os.path.join(TTS_POSITIVE_DIR, d, \"*.wav\")))\n",
    "    for d in os.listdir(TTS_POSITIVE_DIR)\n",
    "    if os.path.isdir(os.path.join(TTS_POSITIVE_DIR, d))\n",
    ")\n",
    "print(f\"\\nTotal positive TTS samples: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial negative samples: phonetically similar Russian words\n",
    "\n",
    "NEGATIVE_PHRASES = [\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0444\\u0435\\u0439\",      # тимофей\n",
    "    \"\\u043a\\u043e\\u0448\\u043a\\u0430\",                    # кошка\n",
    "    \"\\u043c\\u043e\\u0448\\u043a\\u0430\",                    # мошка\n",
    "    \"\\u0440\\u043e\\u043c\\u0430\\u0448\\u043a\\u0430\",      # ромашка\n",
    "    \"\\u043c\\u0430\\u0442\\u0440\\u0451\\u0448\\u043a\\u0430\", # матрёшка\n",
    "    \"\\u0433\\u0430\\u0440\\u043c\\u043e\\u0448\\u043a\\u0430\", # гармошка\n",
    "    \"\\u043a\\u0430\\u0440\\u0442\\u043e\\u0448\\u043a\\u0430\", # картошка\n",
    "    \"\\u043e\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",      # окрошка\n",
    "    \"\\u043c\\u0438\\u0448\\u043a\\u0430\",                    # мишка\n",
    "    \"\\u043c\\u044b\\u0448\\u043a\\u0430\",                    # мышка\n",
    "    \"\\u0442\\u0438\\u0448\\u043a\\u0430\",                    # тишка\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0430\",            # тимоша\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\", # тимошенко\n",
    "    \"\\u043c\\u043e\\u0440\\u043e\\u0448\\u043a\\u0430\",      # морошка\n",
    "    \"\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",            # крошка\n",
    "    \"\\u0434\\u043e\\u0440\\u043e\\u0436\\u043a\\u0430\",      # дорожка\n",
    "    \"\\u043b\\u043e\\u0436\\u043a\\u0430\",                    # ложка\n",
    "    \"\\u0442\\u0438\\u0448\\u0438\\u043d\\u0430\",            # тишина\n",
    "    \"\\u0442\\u0451\\u043c\\u0443\\u0448\\u043a\\u0430\",      # тёмушка\n",
    "]\n",
    "\n",
    "NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE = 50\n",
    "\n",
    "for phrase in NEGATIVE_PHRASES:\n",
    "    for voice_path in voices:\n",
    "        voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n",
    "        safe_phrase = phrase.replace(\"\\u0451\", \"\\u0435\")\n",
    "        out_dir = os.path.join(TTS_NEGATIVE_DIR, f\"{safe_phrase}_{voice_name}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "        if existing >= NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE:\n",
    "            continue\n",
    "\n",
    "        subprocess.run([\n",
    "            \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n",
    "            phrase,\n",
    "            \"--model\", voice_path,\n",
    "            \"--max-samples\", str(NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE),\n",
    "            \"--output-dir\", out_dir,\n",
    "        ], check=True)\n",
    "\n",
    "    print(f\"  Done: {phrase}\")\n",
    "\n",
    "# Count totals\n",
    "total_neg = 0\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    total_neg += len([f for f in files if f.endswith(\".wav\")])\n",
    "\n",
    "print(f\"\\nTotal negative TTS samples: {total_neg}\")\n",
    "print(f\"Expected: {len(NEGATIVE_PHRASES)} phrases x {len(voices)} voices x {NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE} = {len(NEGATIVE_PHRASES) * len(voices) * NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few samples to verify\n",
    "import IPython.display as ipd\n",
    "\n",
    "sample_files = glob.glob(os.path.join(TTS_POSITIVE_DIR, \"*/*.wav\"))[:3]\n",
    "for f in sample_files:\n",
    "    print(f\"Playing: {os.path.basename(os.path.dirname(f))}/{os.path.basename(f)}\")\n",
    "    ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Prepare target voices (Common Voice)\n",
    "\n",
    "**Option A** (below): Download automatically via HuggingFace datasets API.\n",
    "\n",
    "**Option B** (manual):\n",
    "1. Download Russian Common Voice dataset from https://commonvoice.mozilla.org/datasets\n",
    "2. Upload the archive to Google Drive or directly to Colab\n",
    "3. Extract to `/content/common_voice_ru/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download Russian speech samples for voice conversion targets\n# Downloads dev split tar + TSV directly from HuggingFace repo (no datasets lib needed)\n# Uses ffmpeg for MP3 decoding to avoid numpy binary incompatibility issues\n\nimport csv\nimport io\nimport os\nimport random\nimport subprocess\nimport tarfile\n\nimport requests\n\n# Check if we already have enough target voices\nexisting_targets = len(glob.glob(os.path.join(VOICE_TARGETS_DIR, \"*.wav\")))\nif existing_targets >= 100:\n    print(f\"Already have {existing_targets} target voices, skipping download\")\nelse:\n    HF_REPO = \"https://huggingface.co/datasets/fsicoli/common_voice_17_0/resolve/main\"\n    MAX_TARGETS = 100\n\n    # Step 1: Download dev.tsv metadata (~3.7MB)\n    print(\"Downloading dev.tsv metadata...\")\n    tsv_url = f\"{HF_REPO}/transcript/ru/dev.tsv\"\n    tsv_resp = requests.get(tsv_url)\n    tsv_resp.raise_for_status()\n    print(f\"  Downloaded {len(tsv_resp.content) / 1024:.0f} KB\")\n\n    # Parse TSV to find unique speakers and their clips\n    reader = csv.DictReader(io.StringIO(tsv_resp.text), delimiter=\"\\t\")\n    rows = list(reader)\n    print(f\"  {len(rows)} clips in dev split\")\n\n    # Group by speaker (client_id), pick one clip per speaker\n    speaker_clips = {}\n    for row in rows:\n        cid = row[\"client_id\"]\n        if cid not in speaker_clips:\n            speaker_clips[cid] = row[\"path\"]\n\n    print(f\"  {len(speaker_clips)} unique speakers\")\n\n    # Step 2: Download dev tar archive (~375MB)\n    tar_url = f\"{HF_REPO}/audio/ru/dev/ru_dev_0.tar\"\n    tar_path = \"/content/timoshka/ru_dev_0.tar\"\n\n    if not os.path.exists(tar_path):\n        print(f\"Downloading dev audio tar (~375 MB)...\")\n        with requests.get(tar_url, stream=True) as r:\n            r.raise_for_status()\n            total = int(r.headers.get(\"content-length\", 0))\n            downloaded = 0\n            with open(tar_path, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=8 * 1024 * 1024):\n                    f.write(chunk)\n                    downloaded += len(chunk)\n                    if total:\n                        print(f\"\\r  {downloaded / 1024**2:.0f} / {total / 1024**2:.0f} MB\", end=\"\", flush=True)\n        print(f\"\\n  Download complete: {os.path.getsize(tar_path) / 1024**2:.0f} MB\")\n    else:\n        print(f\"Dev tar already downloaded ({os.path.getsize(tar_path) / 1024**2:.0f} MB)\")\n\n    # Step 3: Extract target clips from tar\n    speakers = list(speaker_clips.items())\n    random.shuffle(speakers)\n    wanted_clips = {}\n    for i, (cid, clip_path) in enumerate(speakers):\n        if len(wanted_clips) >= MAX_TARGETS:\n            break\n        wanted_clips[clip_path] = i\n\n    print(f\"Extracting {len(wanted_clips)} clips from tar...\")\n    saved = 0\n    with tarfile.open(tar_path, \"r\") as tar:\n        for member in tar:\n            if not member.isfile():\n                continue\n            basename = os.path.basename(member.name)\n            if basename in wanted_clips:\n                try:\n                    f = tar.extractfile(member)\n                    if f is None:\n                        continue\n                    audio_bytes = f.read()\n\n                    # Save MP3 to temp file\n                    tmp_mp3 = f\"/tmp/cv_clip_{saved}.mp3\"\n                    with open(tmp_mp3, \"wb\") as tmp:\n                        tmp.write(audio_bytes)\n\n                    # Use ffmpeg to convert MP3 -> 16kHz mono WAV (avoids numpy issues)\n                    out_path = os.path.join(VOICE_TARGETS_DIR, f\"speaker_{saved:04d}.wav\")\n                    result = subprocess.run([\n                        \"ffmpeg\", \"-y\", \"-i\", tmp_mp3,\n                        \"-ar\", \"16000\", \"-ac\", \"1\",\n                        \"-f\", \"wav\", out_path\n                    ], capture_output=True, timeout=30)\n\n                    if result.returncode != 0:\n                        continue\n\n                    # Check duration (3-15 seconds) using ffprobe\n                    probe = subprocess.run([\n                        \"ffprobe\", \"-v\", \"error\",\n                        \"-show_entries\", \"format=duration\",\n                        \"-of\", \"csv=p=0\", out_path\n                    ], capture_output=True, text=True, timeout=10)\n\n                    duration = float(probe.stdout.strip())\n                    if duration < 3.0 or duration > 15.0:\n                        os.remove(out_path)\n                        continue\n\n                    saved += 1\n                    os.remove(tmp_mp3)\n\n                except Exception as e:\n                    print(f\"  Error processing {basename}: {e}\")\n\n            if saved >= MAX_TARGETS:\n                break\n\n    print(f\"\\nSaved {saved} target voice files to {VOICE_TARGETS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: If you downloaded Common Voice manually and uploaded to Colab,\n",
    "# uncomment and set the path:\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '/content')\n",
    "# from voice_convert import prepare_common_voice_targets\n",
    "#\n",
    "# prepare_common_voice_targets(\n",
    "#     cv_dir=\"/content/common_voice_ru\",\n",
    "#     output_dir=VOICE_TARGETS_DIR,\n",
    "#     max_clips=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target voice count\n",
    "target_files = sorted(glob.glob(os.path.join(VOICE_TARGETS_DIR, \"*.wav\")))\n",
    "print(f\"Target voice files: {len(target_files)}\")\n",
    "\n",
    "if len(target_files) < 10:\n",
    "    print(\"WARNING: Too few target voices! Aim for 50-100 for good results.\")\n",
    "elif len(target_files) < 50:\n",
    "    print(\"OK: Minimum viable, but 100 targets will produce better results.\")\n",
    "else:\n",
    "    print(\"Good: Sufficient target voices for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Voice Conversion (FreeVC24)\n",
    "\n",
    "This is the longest stage (~3-5 hours on A100).\n",
    "\n",
    "Each TTS sample is converted with each target voice, producing N x M results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Load FreeVC24 model\n",
    "print(\"Loading FreeVC24 model...\")\n",
    "vc_model = TTS(\"voice_conversion_models/multilingual/vctk/freevc24\").to(\"cuda\")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\n\n# Write a standalone VC script that runs as a subprocess (avoids Colab kernel throttling)\nvc_script = r'''#!/usr/bin/env python3\n\"\"\"Standalone voice conversion script — runs as subprocess to avoid Colab kernel throttling.\"\"\"\nimport os, sys, time, json, glob\nfrom pathlib import Path\n\ndef main():\n    config = json.loads(sys.argv[1])\n    source_dir = config[\"source_dir\"]\n    output_dir = config[\"output_dir\"]\n    target_dir = config[\"target_dir\"]\n    label = config.get(\"label\", \"\")\n\n    os.makedirs(output_dir, exist_ok=True)\n    log_path = os.path.join(output_dir, \"_vc_progress.log\")\n    status_path = os.path.join(output_dir, \"_vc_status.json\")\n\n    target_files = sorted(glob.glob(os.path.join(target_dir, \"*.wav\")))\n\n    source_files = []\n    for root, dirs, files in os.walk(source_dir):\n        for f in sorted(files):\n            if f.endswith(\".wav\"):\n                source_files.append(os.path.join(root, f))\n    source_files.sort()\n\n    total = len(source_files) * len(target_files)\n\n    def write_status(done, skipped, errors, t0, finished=False):\n        elapsed = time.time() - t0\n        rate = (done - skipped) / max(elapsed, 1)\n        eta = (total - done) / max(rate, 0.01) if rate > 0.01 else 0\n        status = {\n            \"total\": total, \"done\": done, \"skipped\": skipped, \"errors\": errors,\n            \"elapsed\": elapsed, \"rate\": rate, \"eta\": eta, \"finished\": finished,\n            \"sources\": len(source_files), \"targets\": len(target_files),\n        }\n        with open(status_path + \".tmp\", \"w\") as f:\n            json.dump(status, f)\n        os.replace(status_path + \".tmp\", status_path)\n\n    # Redirect stdout/stderr to log file\n    log_file = open(log_path, \"a\")\n    sys.stdout = log_file\n    sys.stderr = log_file\n\n    print(f\"{label}Loading FreeVC24 model...\")\n    log_file.flush()\n\n    from TTS.api import TTS\n    vc_model = TTS(\"voice_conversion_models/multilingual/vctk/freevc24\").to(\"cuda\")\n    print(f\"{label}Model loaded. Starting conversion: {len(source_files)} sources x {len(target_files)} targets = {total}\")\n    log_file.flush()\n\n    done = 0\n    skipped = 0\n    errors = 0\n    t0 = time.time()\n    write_status(done, skipped, errors, t0)\n\n    for src in source_files:\n        src_name = Path(src).stem\n        parent_name = Path(src).parent.name\n        prefix = f\"{parent_name}_{src_name}\" if parent_name != Path(source_dir).name else src_name\n\n        for tgt in target_files:\n            tgt_name = Path(tgt).stem\n            out_path = os.path.join(output_dir, f\"{prefix}_vc{tgt_name}.wav\")\n\n            if os.path.exists(out_path):\n                skipped += 1\n                done += 1\n                continue\n\n            try:\n                vc_model.voice_conversion_to_file(\n                    source_wav=src, target_wav=tgt, file_path=out_path,\n                )\n                done += 1\n            except Exception as e:\n                errors += 1\n                done += 1\n                if errors <= 10:\n                    print(f\"Error: {prefix} + {tgt_name}: {e}\")\n                    log_file.flush()\n\n            if done % 200 == 0:\n                write_status(done, skipped, errors, t0)\n                elapsed = time.time() - t0\n                rate = (done - skipped) / max(elapsed, 1)\n                eta = (total - done) / max(rate, 0.01)\n                print(f\"[{done}/{total}] {rate:.1f}/s, ETA {eta/60:.0f}min, errors={errors}, skipped={skipped}\")\n                log_file.flush()\n\n    write_status(done, skipped, errors, t0, finished=True)\n    elapsed = time.time() - t0\n    print(f\"DONE: {done - skipped - errors} converted, {skipped} skipped, {errors} errors in {elapsed/60:.1f}min\")\n    log_file.flush()\n    log_file.close()\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nvc_script_path = os.path.join(BASE_DIR, \"run_vc.py\")\nwith open(vc_script_path, \"w\") as f:\n    f.write(vc_script)\n\nprint(f\"Wrote VC subprocess script to {vc_script_path}\")\n\n# Helper to launch and monitor VC subprocess\nimport json\nimport subprocess as sp\n\ndef launch_vc_subprocess(source_dir, output_dir, target_dir, label=\"\"):\n    \"\"\"Launch voice conversion as a separate OS process (immune to Colab throttling).\"\"\"\n    config = json.dumps({\n        \"source_dir\": source_dir,\n        \"output_dir\": output_dir,\n        \"target_dir\": target_dir,\n        \"label\": label,\n    })\n    proc = sp.Popen(\n        [\"python3\", vc_script_path, config],\n        stdout=sp.DEVNULL, stderr=sp.DEVNULL,\n    )\n    print(f\"{label}Launched subprocess PID={proc.pid}\")\n    return proc\n\ndef check_vc_status(output_dir, label=\"\"):\n    \"\"\"Read status from the VC subprocess. Returns (status_dict, finished).\"\"\"\n    status_path = os.path.join(output_dir, \"_vc_status.json\")\n    if not os.path.exists(status_path):\n        return None, False\n    try:\n        with open(status_path) as f:\n            status = json.load(f)\n        done = status[\"done\"]\n        total = status[\"total\"]\n        skipped = status[\"skipped\"]\n        errors = status[\"errors\"]\n        rate = status[\"rate\"]\n        eta = status[\"eta\"]\n        elapsed = status[\"elapsed\"]\n        finished = status.get(\"finished\", False)\n        pct = 100 * done / max(total, 1)\n        bar_len = int(pct / 2)\n        bar = \"#\" * bar_len + \".\" * (50 - bar_len)\n        print(\n            f\"{label}[{bar}] {done}/{total} ({pct:.1f}%) \"\n            f\"| {rate:.1f}/s | ETA {eta/60:.0f}min | \"\n            f\"skip={skipped} err={errors} | {elapsed/60:.1f}min elapsed\"\n        )\n        return status, finished\n    except Exception as e:\n        print(f\"{label}Status read error: {e}\")\n        return None, False\n\nprint(\"VC subprocess launcher ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert positive samples via subprocess (immune to Colab kernel throttling)\nprint(\"=\" * 60)\nprint(\"POSITIVE SAMPLES: Voice Conversion (subprocess)\")\nprint(\"=\" * 60)\n\n# Launch as a completely separate OS process\npos_proc = launch_vc_subprocess(\n    source_dir=TTS_POSITIVE_DIR,\n    output_dir=VC_POSITIVE_DIR,\n    target_dir=VOICE_TARGETS_DIR,\n    label=\"[POSITIVE] \",\n)\n\n# Monitor with short polling (keeps Colab connection alive)\nimport time as _time\n\nwhile True:\n    _time.sleep(30)\n    status, finished = check_vc_status(VC_POSITIVE_DIR, \"[POSITIVE] \")\n    if finished:\n        break\n    # Check if subprocess died\n    if pos_proc.poll() is not None:\n        print(f\"Subprocess exited with code {pos_proc.returncode}\")\n        # Check final status\n        check_vc_status(VC_POSITIVE_DIR, \"[POSITIVE] \")\n        break\n\nn_positive = len(glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\")))\nprint(f\"\\nTotal positive voice-converted samples: {n_positive}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert negative samples via subprocess (immune to Colab kernel throttling)\n# Use 25 target voices (not all 97) to keep runtime under ~2.5 hours\n# 3800 TTS negatives × 25 targets = 95,000 VC negatives\n\nimport random as _random\n\n# Create a subset of target voices for negative VC\nNEG_TARGETS_DIR = os.path.join(BASE_DIR, \"voice_targets_neg25\")\nos.makedirs(NEG_TARGETS_DIR, exist_ok=True)\n\nexisting_neg_targets = len(glob.glob(os.path.join(NEG_TARGETS_DIR, \"*.wav\")))\nif existing_neg_targets < 25:\n    all_targets = sorted(glob.glob(os.path.join(VOICE_TARGETS_DIR, \"*.wav\")))\n    _random.seed(42)  # reproducible subset\n    subset = _random.sample(all_targets, min(25, len(all_targets)))\n    for src in subset:\n        dst = os.path.join(NEG_TARGETS_DIR, os.path.basename(src))\n        if not os.path.exists(dst):\n            os.symlink(src, dst)\n    print(f\"Created negative target subset: {len(subset)} voices\")\nelse:\n    print(f\"Negative target subset already exists: {existing_neg_targets} voices\")\n\nprint(\"=\" * 60)\nprint(\"NEGATIVE SAMPLES: Voice Conversion (subprocess)\")\nprint(\"=\" * 60)\n\nneg_proc = launch_vc_subprocess(\n    source_dir=TTS_NEGATIVE_DIR,\n    output_dir=VC_NEGATIVE_DIR,\n    target_dir=NEG_TARGETS_DIR,\n    label=\"[NEGATIVE] \",\n)\n\nimport time as _time\n\nwhile True:\n    _time.sleep(30)\n    status, finished = check_vc_status(VC_NEGATIVE_DIR, \"[NEGATIVE] \")\n    if finished:\n        break\n    if neg_proc.poll() is not None:\n        print(f\"Subprocess exited with code {neg_proc.returncode}\")\n        check_vc_status(VC_NEGATIVE_DIR, \"[NEGATIVE] \")\n        break\n\nn_negative = len(glob.glob(os.path.join(VC_NEGATIVE_DIR, \"*.wav\")))\nprint(f\"\\nTotal negative voice-converted samples: {n_negative}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample everything to 16kHz mono (safety check)\n",
    "import torchaudio\n",
    "\n",
    "def ensure_16k_mono(directory):\n",
    "    \"\"\"Ensure all WAVs in directory are 16kHz mono.\"\"\"\n",
    "    files = glob.glob(os.path.join(directory, \"*.wav\"))\n",
    "    fixed = 0\n",
    "    for f in files:\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(f)\n",
    "            changed = False\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "                changed = True\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "                changed = True\n",
    "            if changed:\n",
    "                torchaudio.save(f, waveform, 16000)\n",
    "                fixed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Error resampling {f}: {e}\")\n",
    "    return fixed\n",
    "\n",
    "print(\"Checking positive samples...\")\n",
    "fixed_pos = ensure_16k_mono(VC_POSITIVE_DIR)\n",
    "print(f\"  Fixed {fixed_pos} files\")\n",
    "\n",
    "print(\"Checking negative samples...\")\n",
    "fixed_neg = ensure_16k_mono(VC_NEGATIVE_DIR)\n",
    "print(f\"  Fixed {fixed_neg} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few voice-converted samples\n",
    "vc_samples = glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\"))[:3]\n",
    "for f in vc_samples:\n",
    "    print(f\"Playing: {os.path.basename(f)}\")\n",
    "    ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "# ACAV100M features (~6 GB) — negative training data\n",
    "if [ ! -f \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\" ]; then\n",
    "    echo \"Downloading ACAV100M features (~6 GB)...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "else\n",
    "    echo \"ACAV100M features already downloaded\"\n",
    "fi\n",
    "\n",
    "# Validation set (~30 MB)\n",
    "if [ ! -f \"validation_set_features.npy\" ]; then\n",
    "    echo \"Downloading validation set...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "else\n",
    "    echo \"Validation set already downloaded\"\n",
    "fi\n",
    "\n",
    "echo \"\\nData files:\"\n",
    "ls -lh *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "# MIT Room Impulse Responses for reverb augmentation\n",
    "if [ ! -d \"mit_rirs\" ]; then\n",
    "    echo \"Downloading MIT RIRs...\"\n",
    "    mkdir -p mit_rirs\n",
    "    wget -q --show-progress -O mit_rirs.zip \\\n",
    "        https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip\n",
    "    unzip -q mit_rirs.zip -d mit_rirs/ 2>/dev/null || true\n",
    "    rm -f mit_rirs.zip\n",
    "    echo \"MIT RIRs downloaded\"\n",
    "else\n",
    "    echo \"MIT RIRs already present\"\n",
    "fi\n",
    "\n",
    "# Background noise: AudioSet subset + FMA\n",
    "if [ ! -d \"audioset_16k\" ]; then\n",
    "    echo \"Downloading AudioSet background noise subset...\"\n",
    "    mkdir -p audioset_16k\n",
    "    wget -q --show-progress -O audioset_16k.tar.gz \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/audioset_16k_sample.tar.gz \\\n",
    "        2>/dev/null || echo \"Note: AudioSet subset not found. Training will use ACAV100M as primary negative data.\"\n",
    "    if [ -f audioset_16k.tar.gz ]; then\n",
    "        tar -xzf audioset_16k.tar.gz -C audioset_16k/ 2>/dev/null || true\n",
    "        rm -f audioset_16k.tar.gz\n",
    "    fi\n",
    "fi\n",
    "\n",
    "mkdir -p fma\n",
    "echo \"Background data ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Train openWakeWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Count samples\n",
    "n_pos = len(glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\")))\n",
    "n_neg = len(glob.glob(os.path.join(VC_NEGATIVE_DIR, \"*.wav\")))\n",
    "n_pos_tts = sum(\n",
    "    len(glob.glob(os.path.join(TTS_POSITIVE_DIR, d, \"*.wav\")))\n",
    "    for d in os.listdir(TTS_POSITIVE_DIR)\n",
    "    if os.path.isdir(os.path.join(TTS_POSITIVE_DIR, d))\n",
    ")\n",
    "n_neg_tts = 0\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    n_neg_tts += len([f for f in files if f.endswith(\".wav\")])\n",
    "\n",
    "print(f\"Voice-converted positive: {n_pos}\")\n",
    "print(f\"Voice-converted negative: {n_neg}\")\n",
    "print(f\"TTS positive (original):  {n_pos_tts}\")\n",
    "print(f\"TTS negative (original):  {n_neg_tts}\")\n",
    "print(f\"Total positive: {n_pos + n_pos_tts}\")\n",
    "print(f\"Total negative: {n_neg + n_neg_tts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all positive samples into a single directory\n",
    "import shutil\n",
    "\n",
    "ALL_POSITIVE_DIR = os.path.join(BASE_DIR, \"all_positive\")\n",
    "ALL_NEGATIVE_DIR = os.path.join(BASE_DIR, \"all_negative\")\n",
    "os.makedirs(ALL_POSITIVE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_NEGATIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink positive: VC + original TTS\n",
    "for f in glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\")):\n",
    "    dst = os.path.join(ALL_POSITIVE_DIR, os.path.basename(f))\n",
    "    if not os.path.exists(dst):\n",
    "        os.symlink(f, dst)\n",
    "\n",
    "for root, dirs, files in os.walk(TTS_POSITIVE_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            dst = os.path.join(ALL_POSITIVE_DIR, f\"tts_{parent}_{f}\")\n",
    "            if not os.path.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "\n",
    "# Symlink negative: VC + original TTS\n",
    "for f in glob.glob(os.path.join(VC_NEGATIVE_DIR, \"*.wav\")):\n",
    "    dst = os.path.join(ALL_NEGATIVE_DIR, os.path.basename(f))\n",
    "    if not os.path.exists(dst):\n",
    "        os.symlink(f, dst)\n",
    "\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            dst = os.path.join(ALL_NEGATIVE_DIR, f\"tts_{parent}_{f}\")\n",
    "            if not os.path.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "\n",
    "total_pos = len(glob.glob(os.path.join(ALL_POSITIVE_DIR, \"*.wav\")))\n",
    "total_neg = len(glob.glob(os.path.join(ALL_NEGATIVE_DIR, \"*.wav\")))\n",
    "print(f\"All positive samples: {total_pos}\")\n",
    "print(f\"All negative samples: {total_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training config\n",
    "# openWakeWord train.py expects a specific directory structure;\n",
    "# we create the necessary symlinks in output_dir\n",
    "\n",
    "TRAINING_DIR = os.path.join(BASE_DIR, \"training\")\n",
    "os.makedirs(TRAINING_DIR, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"timoshka\",\n",
    "    \"target_phrase\": [\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"],\n",
    "    \"custom_negative_phrases\": [],\n",
    "\n",
    "    # We supply pre-generated clips, set to 0\n",
    "    \"n_samples\": 0,\n",
    "    \"n_samples_val\": 0,\n",
    "\n",
    "    \"augmentation_rounds\": 1,\n",
    "    \"augmentation_batch_size\": 16,\n",
    "\n",
    "    \"piper_sample_generator_path\": \"/content/piper-sample-generator\",\n",
    "    \"output_dir\": TRAINING_DIR,\n",
    "\n",
    "    \"rir_paths\": [os.path.join(BASE_DIR, \"mit_rirs\")],\n",
    "    \"background_paths\": [\n",
    "        os.path.join(BASE_DIR, \"audioset_16k\"),\n",
    "        os.path.join(BASE_DIR, \"fma\"),\n",
    "    ],\n",
    "    \"background_paths_duplication_rate\": [1],\n",
    "\n",
    "    \"feature_data_files\": {\n",
    "        \"ACAV100M_sample\": os.path.join(\n",
    "            BASE_DIR, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "        ),\n",
    "    },\n",
    "    \"false_positive_validation_data_path\": os.path.join(\n",
    "        BASE_DIR, \"validation_set_features.npy\"\n",
    "    ),\n",
    "\n",
    "    \"batch_n_per_class\": {\n",
    "        \"ACAV100M_sample\": 1024,\n",
    "        \"adversarial_negative\": 50,\n",
    "        \"positive\": 50,\n",
    "    },\n",
    "\n",
    "    \"model_type\": \"dnn\",\n",
    "    \"layer_size\": 32,\n",
    "\n",
    "    \"steps\": 50000,\n",
    "    \"max_negative_weight\": 1500,\n",
    "    \"target_false_positives_per_hour\": 0.2,\n",
    "    \"target_accuracy\": 0.7,\n",
    "    \"target_recall\": 0.5,\n",
    "}\n",
    "\n",
    "config_path = os.path.join(BASE_DIR, \"timoshka_config.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"Config saved to {config_path}\")\n",
    "print()\n",
    "print(open(config_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory structure expected by train.py\n",
    "# openWakeWord expects:\n",
    "#   output_dir/<phrase>/positive/  — positive WAVs\n",
    "#   output_dir/<phrase>/negative/  — adversarial negative WAVs\n",
    "\n",
    "phrase_dir = os.path.join(TRAINING_DIR, \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\")\n",
    "pos_link = os.path.join(phrase_dir, \"positive\")\n",
    "neg_link = os.path.join(phrase_dir, \"negative\")\n",
    "\n",
    "os.makedirs(phrase_dir, exist_ok=True)\n",
    "\n",
    "# Create symlinks to our data\n",
    "if os.path.exists(pos_link):\n",
    "    os.remove(pos_link)\n",
    "os.symlink(ALL_POSITIVE_DIR, pos_link)\n",
    "\n",
    "if os.path.exists(neg_link):\n",
    "    os.remove(neg_link)\n",
    "os.symlink(ALL_NEGATIVE_DIR, neg_link)\n",
    "\n",
    "print(f\"Positive -> {pos_link} -> {ALL_POSITIVE_DIR}\")\n",
    "print(f\"Negative -> {neg_link} -> {ALL_NEGATIVE_DIR}\")\n",
    "print(f\"\\nPositive samples: {len(os.listdir(pos_link))}\")\n",
    "print(f\"Negative samples: {len(os.listdir(neg_link))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Step 1: Augmentation (noise, reverb, speed variations)\n",
    "# Skip --generate_clips since we already generated samples externally\n",
    "echo \"Starting augmentation...\"\n",
    "\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --augment_clips \\\n",
    "    --overwrite\n",
    "\n",
    "echo \"\\nAugmentation complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Step 2: Train model\n",
    "echo \"Starting training (this will take a while)...\"\n",
    "\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --train_model\n",
    "\n",
    "echo \"\\nTraining complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Convert ONNX -> TFLite\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --convert_to_tflite\n",
    "\n",
    "echo \"\\nConversion complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and display the result\n",
    "import glob\n",
    "\n",
    "tflite_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.tflite\"), recursive=True)\n",
    "onnx_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.onnx\"), recursive=True)\n",
    "\n",
    "print(\"Generated model files:\")\n",
    "for f in tflite_files + onnx_files:\n",
    "    size = os.path.getsize(f)\n",
    "    print(f\"  {f} ({size/1024:.1f} KB)\")\n",
    "\n",
    "# Copy the tflite to a convenient location\n",
    "if tflite_files:\n",
    "    final_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "    shutil.copy2(tflite_files[0], final_path)\n",
    "    print(f\"\\nFinal model: {final_path}\")\n",
    "    print(f\"Size: {os.path.getsize(final_path)/1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"\\nERROR: No .tflite file found. Check training logs above.\")\n",
    "    if onnx_files:\n",
    "        print(f\"Found ONNX model at: {onnx_files[0]}\")\n",
    "        print(\"Try manual conversion in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fallback: manual ONNX -> TFLite conversion (if automatic conversion failed)\n# Uses onnx2tf instead of deprecated onnx_tf\n\n# import onnx\n# import subprocess\n#\n# onnx_path = onnx_files[0]\n# output_dir_tf = os.path.join(BASE_DIR, \"tf_conversion\")\n#\n# # onnx2tf converts ONNX -> SavedModel -> TFLite in one step\n# subprocess.run([\n#     \"onnx2tf\", \"-i\", onnx_path,\n#     \"-o\", output_dir_tf,\n#     \"-oiqt\",  # output int8 quantized tflite\n# ], check=True)\n#\n# # Find the generated tflite\n# import glob\n# tflite = glob.glob(os.path.join(output_dir_tf, \"**/*.tflite\"), recursive=True)\n# if tflite:\n#     shutil.copy2(tflite[0], os.path.join(BASE_DIR, \"timoshka.tflite\"))\n#     print(f\"Saved: timoshka.tflite ({os.path.getsize(tflite[0])/1024:.1f} KB)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on our samples\n",
    "from openwakeword.model import Model\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "oww_model = Model(wakeword_models=[model_path])\n",
    "model_name = list(oww_model.models.keys())[0]\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "\n",
    "def test_wav(wav_path, model, name):\n",
    "    \"\"\"Test a single WAV file and return max score.\"\"\"\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    audio = (waveform.squeeze().numpy() * 32767).astype(np.int16)\n",
    "\n",
    "    model.reset()\n",
    "    chunk_size = 1280\n",
    "    max_score = 0.0\n",
    "    for i in range(0, len(audio) - chunk_size, chunk_size):\n",
    "        chunk = audio[i:i+chunk_size]\n",
    "        prediction = model.predict(chunk)\n",
    "        score = prediction[name]\n",
    "        max_score = max(max_score, score)\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Test on positive samples\n",
    "print(\"=\" * 50)\n",
    "print(\"POSITIVE SAMPLES (should trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pos_files = glob.glob(os.path.join(ALL_POSITIVE_DIR, \"*.wav\"))\n",
    "test_pos = random.sample(pos_files, min(50, len(pos_files)))\n",
    "\n",
    "pos_scores = []\n",
    "for f in test_pos:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    pos_scores.append(score)\n",
    "\n",
    "triggered = sum(1 for s in pos_scores if s >= 0.5)\n",
    "print(f\"Tested: {len(test_pos)} samples\")\n",
    "print(f\"Triggered (>0.5): {triggered}/{len(test_pos)} ({100*triggered/len(test_pos):.0f}%)\")\n",
    "print(f\"Mean score: {np.mean(pos_scores):.3f}\")\n",
    "print(f\"Min/Max: {np.min(pos_scores):.3f} / {np.max(pos_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on negative samples\n",
    "print(\"=\" * 50)\n",
    "print(\"NEGATIVE SAMPLES (should NOT trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "neg_files = glob.glob(os.path.join(ALL_NEGATIVE_DIR, \"*.wav\"))\n",
    "test_neg = random.sample(neg_files, min(100, len(neg_files)))\n",
    "\n",
    "neg_scores = []\n",
    "false_positives = []\n",
    "for f in test_neg:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    neg_scores.append(score)\n",
    "    if score >= 0.5:\n",
    "        false_positives.append((os.path.basename(f), score))\n",
    "\n",
    "print(f\"Tested: {len(test_neg)} samples\")\n",
    "print(f\"False positives (>0.5): {len(false_positives)}/{len(test_neg)} ({100*len(false_positives)/len(test_neg):.1f}%)\")\n",
    "print(f\"Mean score: {np.mean(neg_scores):.3f}\")\n",
    "print(f\"Min/Max: {np.min(neg_scores):.3f} / {np.max(neg_scores):.3f}\")\n",
    "\n",
    "if false_positives:\n",
    "    print(\"\\nFalse positive files:\")\n",
    "    for fname, score in sorted(false_positives, key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {fname}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tp_rate = sum(1 for s in pos_scores if s >= 0.5) / len(pos_scores) * 100\n",
    "fp_rate = len(false_positives) / len(test_neg) * 100\n",
    "\n",
    "print(f\"True positive rate:  {tp_rate:.0f}%\")\n",
    "print(f\"False positive rate: {fp_rate:.1f}%\")\n",
    "print()\n",
    "\n",
    "if tp_rate >= 70 and fp_rate < 5:\n",
    "    print(\"Model looks good! Ready for deployment.\")\n",
    "elif tp_rate >= 50:\n",
    "    print(\"Model is acceptable. Consider more training or data.\")\n",
    "else:\n",
    "    print(\"Model needs improvement. Try:\")\n",
    "    print(\"  - More diverse target voices\")\n",
    "    print(\"  - More training steps\")\n",
    "    print(\"  - Adjusting threshold (lower for more sensitivity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download via Colab\n",
    "from google.colab import files\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "if os.path.exists(model_path):\n",
    "    files.download(model_path)\n",
    "    print(f\"Downloaded: timoshka.tflite ({os.path.getsize(model_path)/1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Model file not found. Check training logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Home Assistant\n",
    "\n",
    "After downloading `timoshka.tflite`:\n",
    "\n",
    "```bash\n",
    "# 1. Copy model to server\n",
    "scp timoshka.tflite v@plex.local:/home/v/home-assistant/openwakeword-data/timoshka.tflite\n",
    "\n",
    "# 2. The model will be picked up by the openwakeword container via volume mount.\n",
    "#    If using /share/openwakeword/, copy there instead:\n",
    "#    scp timoshka.tflite v@plex.local:/home/v/home-assistant/homeassistant/share/openwakeword/timoshka.tflite\n",
    "\n",
    "# 3. Restart the openwakeword container\n",
    "ssh v@plex.local 'cd /home/v/home-assistant && docker compose restart openwakeword'\n",
    "\n",
    "# 4. In Home Assistant:\n",
    "#    Settings -> Voice assistants -> your assistant -> Wake word -> select \"timoshka\"\n",
    "#\n",
    "# No changes needed to ESPHome/Atom Echo — wake word is processed server-side.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}