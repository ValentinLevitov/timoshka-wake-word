{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training custom wake word \"Тимошка\" for openWakeWord\n\nFull pipeline: Russian Piper TTS → Voice Conversion (FreeVC24) → openWakeWord Training → TFLite\n\n**Requirements:** Google Colab Pro with A100 (recommended). Runtime: 3-6 hours.\n\n## Stages\n1. Install dependencies\n2. Generate TTS samples (Piper, Russian voices)\n3. Prepare target voices from Common Voice\n4. Voice conversion (FreeVC24)\n5. Download negative data (ACAV100M)\n6. Train openWakeWord\n7. Convert to TFLite\n8. Test the model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repositories\n",
    "cd /content\n",
    "\n",
    "if [ ! -d \"openWakeWord\" ]; then\n",
    "    git clone https://github.com/dscripka/openWakeWord.git\n",
    "fi\n",
    "\n",
    "if [ ! -d \"piper-sample-generator\" ]; then\n",
    "    git clone https://github.com/rhasspy/piper-sample-generator.git\n",
    "fi\n",
    "\n",
    "echo \"Done cloning repos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\n# Install dependencies\n# piper-phonemize needs wheels from GitHub releases\npip install -q piper-phonemize -f https://github.com/rhasspy/piper-phonemize/releases/latest\npip install -q webrtcvad\npip install -q -e /content/openWakeWord\npip install -q -e /content/piper-sample-generator\npip install -q coqui-tts\npip install -q mutagen\npip install -q torchinfo\npip install -q torchmetrics\npip install -q speechbrain\npip install -q audiomentations\npip install -q torch-audiomentations\npip install -q acoustics\npip install -q pronouncing\npip install -q datasets\npip install -q deep-phonemizer\n# Use Colab's pre-installed TensorFlow, just add onnx conversion\npip install -q onnx onnx2tf tf2onnx flatbuffers\n\necho \"\\nAll dependencies installed\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Working directories\n",
    "BASE_DIR = \"/content/timoshka\"\n",
    "VOICES_DIR = os.path.join(BASE_DIR, \"piper_voices\")\n",
    "TTS_POSITIVE_DIR = os.path.join(BASE_DIR, \"tts_positive\")\n",
    "TTS_NEGATIVE_DIR = os.path.join(BASE_DIR, \"tts_negative\")\n",
    "VOICE_TARGETS_DIR = os.path.join(BASE_DIR, \"voice_targets\")\n",
    "VC_POSITIVE_DIR = os.path.join(BASE_DIR, \"vc_positive\")\n",
    "VC_NEGATIVE_DIR = os.path.join(BASE_DIR, \"vc_negative\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "\n",
    "for d in [\n",
    "    BASE_DIR, VOICES_DIR, TTS_POSITIVE_DIR, TTS_NEGATIVE_DIR,\n",
    "    VOICE_TARGETS_DIR, VC_POSITIVE_DIR, VC_NEGATIVE_DIR, OUTPUT_DIR,\n",
    "]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Generate TTS samples (Piper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download Russian Piper voices\n",
    "cd /content/timoshka/piper_voices\n",
    "\n",
    "VOICES=(\n",
    "    \"ru/ru_RU/irina/medium/ru_RU-irina-medium\"\n",
    "    \"ru/ru_RU/ruslan/medium/ru_RU-ruslan-medium\"\n",
    "    \"ru/ru_RU/denis/medium/ru_RU-denis-medium\"\n",
    "    \"ru/ru_RU/dmitri/medium/ru_RU-dmitri-medium\"\n",
    ")\n",
    "\n",
    "BASE_URL=\"https://huggingface.co/rhasspy/piper-voices/resolve/main\"\n",
    "\n",
    "for voice in \"${VOICES[@]}\"; do\n",
    "    name=$(basename $voice)\n",
    "    if [ ! -f \"${name}.onnx\" ]; then\n",
    "        echo \"Downloading ${name}...\"\n",
    "        wget -q -O \"${name}.onnx\" \"${BASE_URL}/${voice}.onnx?download=true\"\n",
    "        wget -q -O \"${name}.onnx.json\" \"${BASE_URL}/${voice}.onnx.json?download=true\"\n",
    "    else\n",
    "        echo \"${name} already downloaded\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"\\nAll voices downloaded:\"\n",
    "ls -la *.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "\n",
    "# Positive samples: 250 per voice x 4 voices = 1000 base samples\n",
    "\n",
    "voices = sorted(glob.glob(os.path.join(VOICES_DIR, \"*.onnx\")))\n",
    "print(f\"Found {len(voices)} Piper voices\")\n",
    "\n",
    "POSITIVE_PHRASE = \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"  # тимошка\n",
    "SAMPLES_PER_VOICE = 250\n",
    "\n",
    "for voice_path in voices:\n",
    "    voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n",
    "    out_dir = os.path.join(TTS_POSITIVE_DIR, voice_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "    if existing >= SAMPLES_PER_VOICE:\n",
    "        print(f\"  {voice_name}: {existing} samples already exist, skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Generating {SAMPLES_PER_VOICE} positive samples with {voice_name}...\")\n",
    "    subprocess.run([\n",
    "        \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n",
    "        POSITIVE_PHRASE,\n",
    "        \"--model\", voice_path,\n",
    "        \"--max-samples\", str(SAMPLES_PER_VOICE),\n",
    "        \"--output-dir\", out_dir,\n",
    "    ], check=True)\n",
    "\n",
    "    generated = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "    print(f\"    Generated: {generated}\")\n",
    "\n",
    "total = sum(\n",
    "    len(glob.glob(os.path.join(TTS_POSITIVE_DIR, d, \"*.wav\")))\n",
    "    for d in os.listdir(TTS_POSITIVE_DIR)\n",
    "    if os.path.isdir(os.path.join(TTS_POSITIVE_DIR, d))\n",
    ")\n",
    "print(f\"\\nTotal positive TTS samples: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial negative samples: phonetically similar Russian words\n",
    "\n",
    "NEGATIVE_PHRASES = [\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0444\\u0435\\u0439\",      # тимофей\n",
    "    \"\\u043a\\u043e\\u0448\\u043a\\u0430\",                    # кошка\n",
    "    \"\\u043c\\u043e\\u0448\\u043a\\u0430\",                    # мошка\n",
    "    \"\\u0440\\u043e\\u043c\\u0430\\u0448\\u043a\\u0430\",      # ромашка\n",
    "    \"\\u043c\\u0430\\u0442\\u0440\\u0451\\u0448\\u043a\\u0430\", # матрёшка\n",
    "    \"\\u0433\\u0430\\u0440\\u043c\\u043e\\u0448\\u043a\\u0430\", # гармошка\n",
    "    \"\\u043a\\u0430\\u0440\\u0442\\u043e\\u0448\\u043a\\u0430\", # картошка\n",
    "    \"\\u043e\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",      # окрошка\n",
    "    \"\\u043c\\u0438\\u0448\\u043a\\u0430\",                    # мишка\n",
    "    \"\\u043c\\u044b\\u0448\\u043a\\u0430\",                    # мышка\n",
    "    \"\\u0442\\u0438\\u0448\\u043a\\u0430\",                    # тишка\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0430\",            # тимоша\n",
    "    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\", # тимошенко\n",
    "    \"\\u043c\\u043e\\u0440\\u043e\\u0448\\u043a\\u0430\",      # морошка\n",
    "    \"\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",            # крошка\n",
    "    \"\\u0434\\u043e\\u0440\\u043e\\u0436\\u043a\\u0430\",      # дорожка\n",
    "    \"\\u043b\\u043e\\u0436\\u043a\\u0430\",                    # ложка\n",
    "    \"\\u0442\\u0438\\u0448\\u0438\\u043d\\u0430\",            # тишина\n",
    "    \"\\u0442\\u0451\\u043c\\u0443\\u0448\\u043a\\u0430\",      # тёмушка\n",
    "]\n",
    "\n",
    "NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE = 50\n",
    "\n",
    "for phrase in NEGATIVE_PHRASES:\n",
    "    for voice_path in voices:\n",
    "        voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n",
    "        safe_phrase = phrase.replace(\"\\u0451\", \"\\u0435\")\n",
    "        out_dir = os.path.join(TTS_NEGATIVE_DIR, f\"{safe_phrase}_{voice_name}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n",
    "        if existing >= NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE:\n",
    "            continue\n",
    "\n",
    "        subprocess.run([\n",
    "            \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n",
    "            phrase,\n",
    "            \"--model\", voice_path,\n",
    "            \"--max-samples\", str(NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE),\n",
    "            \"--output-dir\", out_dir,\n",
    "        ], check=True)\n",
    "\n",
    "    print(f\"  Done: {phrase}\")\n",
    "\n",
    "# Count totals\n",
    "total_neg = 0\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    total_neg += len([f for f in files if f.endswith(\".wav\")])\n",
    "\n",
    "print(f\"\\nTotal negative TTS samples: {total_neg}\")\n",
    "print(f\"Expected: {len(NEGATIVE_PHRASES)} phrases x {len(voices)} voices x {NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE} = {len(NEGATIVE_PHRASES) * len(voices) * NEGATIVE_SAMPLES_PER_PHRASE_PER_VOICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few samples to verify\n",
    "import IPython.display as ipd\n",
    "\n",
    "sample_files = glob.glob(os.path.join(TTS_POSITIVE_DIR, \"*/*.wav\"))[:3]\n",
    "for f in sample_files:\n",
    "    print(f\"Playing: {os.path.basename(os.path.dirname(f))}/{os.path.basename(f)}\")\n",
    "    ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Prepare target voices (Common Voice)\n",
    "\n",
    "**Option A** (below): Download automatically via HuggingFace datasets API.\n",
    "\n",
    "**Option B** (manual):\n",
    "1. Download Russian Common Voice dataset from https://commonvoice.mozilla.org/datasets\n",
    "2. Upload the archive to Google Drive or directly to Colab\n",
    "3. Extract to `/content/common_voice_ru/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option A: Download via HuggingFace datasets API\n\nfrom datasets import load_dataset\nimport soundfile as sf\nimport random\n\nprint(\"Loading Russian Common Voice from HuggingFace...\")\nprint(\"(This may take a while on first download)\")\n\ncv_dataset = load_dataset(\n    \"mozilla-foundation/common_voice_16_1\",\n    \"ru\",\n    split=\"validated\",\n    trust_remote_code=True,\n)\n\nprint(f\"Total validated clips: {len(cv_dataset)}\")\n\n# Pick one clip per unique client_id for speaker diversity\nseen_speakers = set()\nselected = []\n\nindices = list(range(len(cv_dataset)))\nrandom.shuffle(indices)\n\nMAX_TARGETS = 100\n\nfor idx in indices:\n    if len(selected) >= MAX_TARGETS:\n        break\n    row = cv_dataset[idx]\n    speaker = row[\"client_id\"]\n    if speaker in seen_speakers:\n        continue\n\n    audio = row[\"audio\"]\n    duration = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n\n    # Filter: 3-15 seconds\n    if duration < 3.0 or duration > 15.0:\n        continue\n\n    seen_speakers.add(speaker)\n    selected.append(row)\n\nprint(f\"Selected {len(selected)} unique speakers\")\n\n# Save as 16kHz mono WAV\nimport torchaudio\nimport torch\n\nfor i, row in enumerate(selected):\n    audio = row[\"audio\"]\n    waveform = torch.tensor(audio[\"array\"]).unsqueeze(0).float()\n    sr = audio[\"sampling_rate\"]\n\n    if sr != 16000:\n        resampler = torchaudio.transforms.Resample(sr, 16000)\n        waveform = resampler(waveform)\n\n    out_path = os.path.join(VOICE_TARGETS_DIR, f\"speaker_{i:04d}.wav\")\n    torchaudio.save(out_path, waveform, 16000)\n\nprint(f\"Saved {len(selected)} target voice files to {VOICE_TARGETS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: If you downloaded Common Voice manually and uploaded to Colab,\n",
    "# uncomment and set the path:\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '/content')\n",
    "# from voice_convert import prepare_common_voice_targets\n",
    "#\n",
    "# prepare_common_voice_targets(\n",
    "#     cv_dir=\"/content/common_voice_ru\",\n",
    "#     output_dir=VOICE_TARGETS_DIR,\n",
    "#     max_clips=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target voice count\n",
    "target_files = sorted(glob.glob(os.path.join(VOICE_TARGETS_DIR, \"*.wav\")))\n",
    "print(f\"Target voice files: {len(target_files)}\")\n",
    "\n",
    "if len(target_files) < 10:\n",
    "    print(\"WARNING: Too few target voices! Aim for 50-100 for good results.\")\n",
    "elif len(target_files) < 50:\n",
    "    print(\"OK: Minimum viable, but 100 targets will produce better results.\")\n",
    "else:\n",
    "    print(\"Good: Sufficient target voices for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Voice Conversion (FreeVC24)\n",
    "\n",
    "This is the longest stage (~3-5 hours on A100).\n",
    "\n",
    "Each TTS sample is converted with each target voice, producing N x M results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Load FreeVC24 model\n",
    "print(\"Loading FreeVC24 model...\")\n",
    "vc_model = TTS(\"voice_conversion_models/multilingual/vctk/freevc24\").to(\"cuda\")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_voice_conversion(source_dir, output_dir, target_files, label=\"\"):\n",
    "    \"\"\"Convert all WAVs in source_dir with all target voices.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Collect all source WAVs (may be in subdirectories)\n",
    "    source_files = []\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".wav\"):\n",
    "                source_files.append(os.path.join(root, f))\n",
    "    source_files.sort()\n",
    "\n",
    "    total = len(source_files) * len(target_files)\n",
    "    print(f\"{label}Sources: {len(source_files)}, Targets: {len(target_files)}, Total: {total}\")\n",
    "\n",
    "    done = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for src in source_files:\n",
    "        src_name = Path(src).stem\n",
    "        # Include parent dir name to avoid collisions\n",
    "        parent_name = Path(src).parent.name\n",
    "        prefix = f\"{parent_name}_{src_name}\" if parent_name != Path(source_dir).name else src_name\n",
    "\n",
    "        for tgt in target_files:\n",
    "            tgt_name = Path(tgt).stem\n",
    "            out_path = os.path.join(output_dir, f\"{prefix}_vc{tgt_name}.wav\")\n",
    "\n",
    "            if os.path.exists(out_path):\n",
    "                skipped += 1\n",
    "                done += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                vc_model.voice_conversion_to_file(\n",
    "                    source_wav=src,\n",
    "                    target_wav=tgt,\n",
    "                    file_path=out_path,\n",
    "                )\n",
    "                done += 1\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                if errors <= 5:\n",
    "                    print(f\"  Error: {prefix} + {tgt_name}: {e}\")\n",
    "\n",
    "            if done % 500 == 0:\n",
    "                elapsed = time.time() - t0\n",
    "                rate = (done - skipped) / max(elapsed, 1)\n",
    "                eta = (total - done) / max(rate, 0.01)\n",
    "                print(\n",
    "                    f\"  [{done}/{total}] {rate:.1f}/s, \"\n",
    "                    f\"ETA {eta/60:.0f}min, errors={errors}\"\n",
    "                )\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(\n",
    "        f\"  Done: {done - skipped - errors} converted, \"\n",
    "        f\"{skipped} skipped, {errors} errors in {elapsed/60:.1f}min\\n\"\n",
    "    )\n",
    "    return done - skipped - errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert positive samples\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITIVE SAMPLES: Voice Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_positive = run_voice_conversion(\n",
    "    source_dir=TTS_POSITIVE_DIR,\n",
    "    output_dir=VC_POSITIVE_DIR,\n",
    "    target_files=target_files,\n",
    "    label=\"[POSITIVE] \",\n",
    ")\n",
    "\n",
    "print(f\"Total positive voice-converted samples: {n_positive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negative samples\n",
    "print(\"=\" * 60)\n",
    "print(\"NEGATIVE SAMPLES: Voice Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_negative = run_voice_conversion(\n",
    "    source_dir=TTS_NEGATIVE_DIR,\n",
    "    output_dir=VC_NEGATIVE_DIR,\n",
    "    target_files=target_files,\n",
    "    label=\"[NEGATIVE] \",\n",
    ")\n",
    "\n",
    "print(f\"Total negative voice-converted samples: {n_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample everything to 16kHz mono (safety check)\n",
    "import torchaudio\n",
    "\n",
    "def ensure_16k_mono(directory):\n",
    "    \"\"\"Ensure all WAVs in directory are 16kHz mono.\"\"\"\n",
    "    files = glob.glob(os.path.join(directory, \"*.wav\"))\n",
    "    fixed = 0\n",
    "    for f in files:\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(f)\n",
    "            changed = False\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "                changed = True\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "                changed = True\n",
    "            if changed:\n",
    "                torchaudio.save(f, waveform, 16000)\n",
    "                fixed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Error resampling {f}: {e}\")\n",
    "    return fixed\n",
    "\n",
    "print(\"Checking positive samples...\")\n",
    "fixed_pos = ensure_16k_mono(VC_POSITIVE_DIR)\n",
    "print(f\"  Fixed {fixed_pos} files\")\n",
    "\n",
    "print(\"Checking negative samples...\")\n",
    "fixed_neg = ensure_16k_mono(VC_NEGATIVE_DIR)\n",
    "print(f\"  Fixed {fixed_neg} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few voice-converted samples\n",
    "vc_samples = glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\"))[:3]\n",
    "for f in vc_samples:\n",
    "    print(f\"Playing: {os.path.basename(f)}\")\n",
    "    ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "# ACAV100M features (~6 GB) — negative training data\n",
    "if [ ! -f \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\" ]; then\n",
    "    echo \"Downloading ACAV100M features (~6 GB)...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "else\n",
    "    echo \"ACAV100M features already downloaded\"\n",
    "fi\n",
    "\n",
    "# Validation set (~30 MB)\n",
    "if [ ! -f \"validation_set_features.npy\" ]; then\n",
    "    echo \"Downloading validation set...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "else\n",
    "    echo \"Validation set already downloaded\"\n",
    "fi\n",
    "\n",
    "echo \"\\nData files:\"\n",
    "ls -lh *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "# MIT Room Impulse Responses for reverb augmentation\n",
    "if [ ! -d \"mit_rirs\" ]; then\n",
    "    echo \"Downloading MIT RIRs...\"\n",
    "    mkdir -p mit_rirs\n",
    "    wget -q --show-progress -O mit_rirs.zip \\\n",
    "        https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip\n",
    "    unzip -q mit_rirs.zip -d mit_rirs/ 2>/dev/null || true\n",
    "    rm -f mit_rirs.zip\n",
    "    echo \"MIT RIRs downloaded\"\n",
    "else\n",
    "    echo \"MIT RIRs already present\"\n",
    "fi\n",
    "\n",
    "# Background noise: AudioSet subset + FMA\n",
    "if [ ! -d \"audioset_16k\" ]; then\n",
    "    echo \"Downloading AudioSet background noise subset...\"\n",
    "    mkdir -p audioset_16k\n",
    "    wget -q --show-progress -O audioset_16k.tar.gz \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/audioset_16k_sample.tar.gz \\\n",
    "        2>/dev/null || echo \"Note: AudioSet subset not found. Training will use ACAV100M as primary negative data.\"\n",
    "    if [ -f audioset_16k.tar.gz ]; then\n",
    "        tar -xzf audioset_16k.tar.gz -C audioset_16k/ 2>/dev/null || true\n",
    "        rm -f audioset_16k.tar.gz\n",
    "    fi\n",
    "fi\n",
    "\n",
    "mkdir -p fma\n",
    "echo \"Background data ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Train openWakeWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Count samples\n",
    "n_pos = len(glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\")))\n",
    "n_neg = len(glob.glob(os.path.join(VC_NEGATIVE_DIR, \"*.wav\")))\n",
    "n_pos_tts = sum(\n",
    "    len(glob.glob(os.path.join(TTS_POSITIVE_DIR, d, \"*.wav\")))\n",
    "    for d in os.listdir(TTS_POSITIVE_DIR)\n",
    "    if os.path.isdir(os.path.join(TTS_POSITIVE_DIR, d))\n",
    ")\n",
    "n_neg_tts = 0\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    n_neg_tts += len([f for f in files if f.endswith(\".wav\")])\n",
    "\n",
    "print(f\"Voice-converted positive: {n_pos}\")\n",
    "print(f\"Voice-converted negative: {n_neg}\")\n",
    "print(f\"TTS positive (original):  {n_pos_tts}\")\n",
    "print(f\"TTS negative (original):  {n_neg_tts}\")\n",
    "print(f\"Total positive: {n_pos + n_pos_tts}\")\n",
    "print(f\"Total negative: {n_neg + n_neg_tts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all positive samples into a single directory\n",
    "import shutil\n",
    "\n",
    "ALL_POSITIVE_DIR = os.path.join(BASE_DIR, \"all_positive\")\n",
    "ALL_NEGATIVE_DIR = os.path.join(BASE_DIR, \"all_negative\")\n",
    "os.makedirs(ALL_POSITIVE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_NEGATIVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink positive: VC + original TTS\n",
    "for f in glob.glob(os.path.join(VC_POSITIVE_DIR, \"*.wav\")):\n",
    "    dst = os.path.join(ALL_POSITIVE_DIR, os.path.basename(f))\n",
    "    if not os.path.exists(dst):\n",
    "        os.symlink(f, dst)\n",
    "\n",
    "for root, dirs, files in os.walk(TTS_POSITIVE_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            dst = os.path.join(ALL_POSITIVE_DIR, f\"tts_{parent}_{f}\")\n",
    "            if not os.path.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "\n",
    "# Symlink negative: VC + original TTS\n",
    "for f in glob.glob(os.path.join(VC_NEGATIVE_DIR, \"*.wav\")):\n",
    "    dst = os.path.join(ALL_NEGATIVE_DIR, os.path.basename(f))\n",
    "    if not os.path.exists(dst):\n",
    "        os.symlink(f, dst)\n",
    "\n",
    "for root, dirs, files in os.walk(TTS_NEGATIVE_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".wav\"):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            dst = os.path.join(ALL_NEGATIVE_DIR, f\"tts_{parent}_{f}\")\n",
    "            if not os.path.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "\n",
    "total_pos = len(glob.glob(os.path.join(ALL_POSITIVE_DIR, \"*.wav\")))\n",
    "total_neg = len(glob.glob(os.path.join(ALL_NEGATIVE_DIR, \"*.wav\")))\n",
    "print(f\"All positive samples: {total_pos}\")\n",
    "print(f\"All negative samples: {total_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training config\n",
    "# openWakeWord train.py expects a specific directory structure;\n",
    "# we create the necessary symlinks in output_dir\n",
    "\n",
    "TRAINING_DIR = os.path.join(BASE_DIR, \"training\")\n",
    "os.makedirs(TRAINING_DIR, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"timoshka\",\n",
    "    \"target_phrase\": [\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"],\n",
    "    \"custom_negative_phrases\": [],\n",
    "\n",
    "    # We supply pre-generated clips, set to 0\n",
    "    \"n_samples\": 0,\n",
    "    \"n_samples_val\": 0,\n",
    "\n",
    "    \"augmentation_rounds\": 1,\n",
    "    \"augmentation_batch_size\": 16,\n",
    "\n",
    "    \"piper_sample_generator_path\": \"/content/piper-sample-generator\",\n",
    "    \"output_dir\": TRAINING_DIR,\n",
    "\n",
    "    \"rir_paths\": [os.path.join(BASE_DIR, \"mit_rirs\")],\n",
    "    \"background_paths\": [\n",
    "        os.path.join(BASE_DIR, \"audioset_16k\"),\n",
    "        os.path.join(BASE_DIR, \"fma\"),\n",
    "    ],\n",
    "    \"background_paths_duplication_rate\": [1],\n",
    "\n",
    "    \"feature_data_files\": {\n",
    "        \"ACAV100M_sample\": os.path.join(\n",
    "            BASE_DIR, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "        ),\n",
    "    },\n",
    "    \"false_positive_validation_data_path\": os.path.join(\n",
    "        BASE_DIR, \"validation_set_features.npy\"\n",
    "    ),\n",
    "\n",
    "    \"batch_n_per_class\": {\n",
    "        \"ACAV100M_sample\": 1024,\n",
    "        \"adversarial_negative\": 50,\n",
    "        \"positive\": 50,\n",
    "    },\n",
    "\n",
    "    \"model_type\": \"dnn\",\n",
    "    \"layer_size\": 32,\n",
    "\n",
    "    \"steps\": 50000,\n",
    "    \"max_negative_weight\": 1500,\n",
    "    \"target_false_positives_per_hour\": 0.2,\n",
    "    \"target_accuracy\": 0.7,\n",
    "    \"target_recall\": 0.5,\n",
    "}\n",
    "\n",
    "config_path = os.path.join(BASE_DIR, \"timoshka_config.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"Config saved to {config_path}\")\n",
    "print()\n",
    "print(open(config_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory structure expected by train.py\n",
    "# openWakeWord expects:\n",
    "#   output_dir/<phrase>/positive/  — positive WAVs\n",
    "#   output_dir/<phrase>/negative/  — adversarial negative WAVs\n",
    "\n",
    "phrase_dir = os.path.join(TRAINING_DIR, \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\")\n",
    "pos_link = os.path.join(phrase_dir, \"positive\")\n",
    "neg_link = os.path.join(phrase_dir, \"negative\")\n",
    "\n",
    "os.makedirs(phrase_dir, exist_ok=True)\n",
    "\n",
    "# Create symlinks to our data\n",
    "if os.path.exists(pos_link):\n",
    "    os.remove(pos_link)\n",
    "os.symlink(ALL_POSITIVE_DIR, pos_link)\n",
    "\n",
    "if os.path.exists(neg_link):\n",
    "    os.remove(neg_link)\n",
    "os.symlink(ALL_NEGATIVE_DIR, neg_link)\n",
    "\n",
    "print(f\"Positive -> {pos_link} -> {ALL_POSITIVE_DIR}\")\n",
    "print(f\"Negative -> {neg_link} -> {ALL_NEGATIVE_DIR}\")\n",
    "print(f\"\\nPositive samples: {len(os.listdir(pos_link))}\")\n",
    "print(f\"Negative samples: {len(os.listdir(neg_link))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Step 1: Augmentation (noise, reverb, speed variations)\n",
    "# Skip --generate_clips since we already generated samples externally\n",
    "echo \"Starting augmentation...\"\n",
    "\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --augment_clips \\\n",
    "    --overwrite\n",
    "\n",
    "echo \"\\nAugmentation complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Step 2: Train model\n",
    "echo \"Starting training (this will take a while)...\"\n",
    "\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --train_model\n",
    "\n",
    "echo \"\\nTraining complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content\n",
    "\n",
    "# Convert ONNX -> TFLite\n",
    "python openWakeWord/openwakeword/train.py \\\n",
    "    --training_config /content/timoshka/timoshka_config.yaml \\\n",
    "    --convert_to_tflite\n",
    "\n",
    "echo \"\\nConversion complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and display the result\n",
    "import glob\n",
    "\n",
    "tflite_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.tflite\"), recursive=True)\n",
    "onnx_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.onnx\"), recursive=True)\n",
    "\n",
    "print(\"Generated model files:\")\n",
    "for f in tflite_files + onnx_files:\n",
    "    size = os.path.getsize(f)\n",
    "    print(f\"  {f} ({size/1024:.1f} KB)\")\n",
    "\n",
    "# Copy the tflite to a convenient location\n",
    "if tflite_files:\n",
    "    final_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "    shutil.copy2(tflite_files[0], final_path)\n",
    "    print(f\"\\nFinal model: {final_path}\")\n",
    "    print(f\"Size: {os.path.getsize(final_path)/1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"\\nERROR: No .tflite file found. Check training logs above.\")\n",
    "    if onnx_files:\n",
    "        print(f\"Found ONNX model at: {onnx_files[0]}\")\n",
    "        print(\"Try manual conversion in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fallback: manual ONNX -> TFLite conversion (if automatic conversion failed)\n# Uses onnx2tf instead of deprecated onnx_tf\n\n# import onnx\n# import subprocess\n#\n# onnx_path = onnx_files[0]\n# output_dir_tf = os.path.join(BASE_DIR, \"tf_conversion\")\n#\n# # onnx2tf converts ONNX -> SavedModel -> TFLite in one step\n# subprocess.run([\n#     \"onnx2tf\", \"-i\", onnx_path,\n#     \"-o\", output_dir_tf,\n#     \"-oiqt\",  # output int8 quantized tflite\n# ], check=True)\n#\n# # Find the generated tflite\n# import glob\n# tflite = glob.glob(os.path.join(output_dir_tf, \"**/*.tflite\"), recursive=True)\n# if tflite:\n#     shutil.copy2(tflite[0], os.path.join(BASE_DIR, \"timoshka.tflite\"))\n#     print(f\"Saved: timoshka.tflite ({os.path.getsize(tflite[0])/1024:.1f} KB)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on our samples\n",
    "from openwakeword.model import Model\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "oww_model = Model(wakeword_models=[model_path])\n",
    "model_name = list(oww_model.models.keys())[0]\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "\n",
    "def test_wav(wav_path, model, name):\n",
    "    \"\"\"Test a single WAV file and return max score.\"\"\"\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    audio = (waveform.squeeze().numpy() * 32767).astype(np.int16)\n",
    "\n",
    "    model.reset()\n",
    "    chunk_size = 1280\n",
    "    max_score = 0.0\n",
    "    for i in range(0, len(audio) - chunk_size, chunk_size):\n",
    "        chunk = audio[i:i+chunk_size]\n",
    "        prediction = model.predict(chunk)\n",
    "        score = prediction[name]\n",
    "        max_score = max(max_score, score)\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Test on positive samples\n",
    "print(\"=\" * 50)\n",
    "print(\"POSITIVE SAMPLES (should trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pos_files = glob.glob(os.path.join(ALL_POSITIVE_DIR, \"*.wav\"))\n",
    "test_pos = random.sample(pos_files, min(50, len(pos_files)))\n",
    "\n",
    "pos_scores = []\n",
    "for f in test_pos:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    pos_scores.append(score)\n",
    "\n",
    "triggered = sum(1 for s in pos_scores if s >= 0.5)\n",
    "print(f\"Tested: {len(test_pos)} samples\")\n",
    "print(f\"Triggered (>0.5): {triggered}/{len(test_pos)} ({100*triggered/len(test_pos):.0f}%)\")\n",
    "print(f\"Mean score: {np.mean(pos_scores):.3f}\")\n",
    "print(f\"Min/Max: {np.min(pos_scores):.3f} / {np.max(pos_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on negative samples\n",
    "print(\"=\" * 50)\n",
    "print(\"NEGATIVE SAMPLES (should NOT trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "neg_files = glob.glob(os.path.join(ALL_NEGATIVE_DIR, \"*.wav\"))\n",
    "test_neg = random.sample(neg_files, min(100, len(neg_files)))\n",
    "\n",
    "neg_scores = []\n",
    "false_positives = []\n",
    "for f in test_neg:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    neg_scores.append(score)\n",
    "    if score >= 0.5:\n",
    "        false_positives.append((os.path.basename(f), score))\n",
    "\n",
    "print(f\"Tested: {len(test_neg)} samples\")\n",
    "print(f\"False positives (>0.5): {len(false_positives)}/{len(test_neg)} ({100*len(false_positives)/len(test_neg):.1f}%)\")\n",
    "print(f\"Mean score: {np.mean(neg_scores):.3f}\")\n",
    "print(f\"Min/Max: {np.min(neg_scores):.3f} / {np.max(neg_scores):.3f}\")\n",
    "\n",
    "if false_positives:\n",
    "    print(\"\\nFalse positive files:\")\n",
    "    for fname, score in sorted(false_positives, key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {fname}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tp_rate = sum(1 for s in pos_scores if s >= 0.5) / len(pos_scores) * 100\n",
    "fp_rate = len(false_positives) / len(test_neg) * 100\n",
    "\n",
    "print(f\"True positive rate:  {tp_rate:.0f}%\")\n",
    "print(f\"False positive rate: {fp_rate:.1f}%\")\n",
    "print()\n",
    "\n",
    "if tp_rate >= 70 and fp_rate < 5:\n",
    "    print(\"Model looks good! Ready for deployment.\")\n",
    "elif tp_rate >= 50:\n",
    "    print(\"Model is acceptable. Consider more training or data.\")\n",
    "else:\n",
    "    print(\"Model needs improvement. Try:\")\n",
    "    print(\"  - More diverse target voices\")\n",
    "    print(\"  - More training steps\")\n",
    "    print(\"  - Adjusting threshold (lower for more sensitivity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download via Colab\n",
    "from google.colab import files\n",
    "\n",
    "model_path = os.path.join(BASE_DIR, \"timoshka.tflite\")\n",
    "if os.path.exists(model_path):\n",
    "    files.download(model_path)\n",
    "    print(f\"Downloaded: timoshka.tflite ({os.path.getsize(model_path)/1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Model file not found. Check training logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Home Assistant\n",
    "\n",
    "After downloading `timoshka.tflite`:\n",
    "\n",
    "```bash\n",
    "# 1. Copy model to server\n",
    "scp timoshka.tflite v@plex.local:/home/v/home-assistant/openwakeword-data/timoshka.tflite\n",
    "\n",
    "# 2. The model will be picked up by the openwakeword container via volume mount.\n",
    "#    If using /share/openwakeword/, copy there instead:\n",
    "#    scp timoshka.tflite v@plex.local:/home/v/home-assistant/homeassistant/share/openwakeword/timoshka.tflite\n",
    "\n",
    "# 3. Restart the openwakeword container\n",
    "ssh v@plex.local 'cd /home/v/home-assistant && docker compose restart openwakeword'\n",
    "\n",
    "# 4. In Home Assistant:\n",
    "#    Settings -> Voice assistants -> your assistant -> Wake word -> select \"timoshka\"\n",
    "#\n",
    "# No changes needed to ESPHome/Atom Echo — wake word is processed server-side.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}