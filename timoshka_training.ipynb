{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training custom wake word \"Тимошка\" for openWakeWord\n\nFull pipeline: Russian Piper TTS → Voice Conversion (FreeVC24) → openWakeWord Training → TFLite\n\n**Requirements:** Google Colab with GPU (A100 recommended). Google Drive with ~20 GB free space.\n\n**Key feature:** All intermediate results are saved to Google Drive. If the runtime disconnects,\njust reconnect and re-run — everything resumes from where it left off. Maximum data loss: ~1 target voice (~5 min of work).\n\n**TEST_MODE:** Set `TEST_MODE = True` for a ~5 minute end-to-end pipeline validation\n(1 voice, 5 samples, 2 neg phrases, 1 target, 100 training steps).\n\n## Stages\n1. Mount Google Drive & check space\n2. Install dependencies\n3. Generate TTS samples (Piper, Russian voices)\n4. Prepare target voices from Common Voice\n5. Voice conversion (FreeVC24) — with Google Drive checkpointing\n6. Download negative data (ACAV100M)\n7. Train openWakeWord\n8. Convert to TFLite\n9. Test the model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Configuration { display-mode: \"form\" }\n\n#@markdown ### Test mode (quick end-to-end validation, ~5 min)\nTEST_MODE = False  #@param {type:\"boolean\"}\n\n#@markdown ### Number of target voices for voice conversion\n#@markdown (ignored in TEST_MODE, which uses 1)\nN_TARGET_VOICES = 100  #@param {type:\"integer\"}\n\n#@markdown ### Training steps (ignored in TEST_MODE, which uses 100)\nTRAINING_STEPS = 50000  #@param {type:\"integer\"}\n\nif TEST_MODE:\n    N_TARGET_VOICES = 1\n    TRAINING_STEPS = 100\n    N_VOICES = 1           # 1 Piper voice instead of 4\n    SAMPLES_PER_VOICE = 5  # 5 samples instead of 250\n    NEG_PHRASES_COUNT = 2  # 2 negative phrases instead of 19\n    NEG_SAMPLES_PER = 5    # 5 negative samples instead of 50\n    print(\"=\" * 60)\n    print(f\"TEST MODE: {N_TARGET_VOICES} target, {TRAINING_STEPS} steps\")\n    print(f\"  {N_VOICES} voice, {SAMPLES_PER_VOICE} pos samples, {NEG_PHRASES_COUNT} neg phrases\")\n    print(\"Expected runtime: ~5 minutes\")\n    print(\"=\" * 60)\nelse:\n    N_VOICES = 4\n    SAMPLES_PER_VOICE = 250\n    NEG_PHRASES_COUNT = 19\n    NEG_SAMPLES_PER = 50\n    print(f\"FULL MODE: {N_TARGET_VOICES} target voices, {TRAINING_STEPS} training steps\")\n    print(f\"Expected runtime: 4-8 hours on A100\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Mount Google Drive & check space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check available space\n",
    "usage = shutil.disk_usage('/content/drive')\n",
    "free_gb = usage.free / (1024**3)\n",
    "total_gb = usage.total / (1024**3)\n",
    "used_gb = usage.used / (1024**3)\n",
    "\n",
    "print(f\"Google Drive: {used_gb:.1f} GB used / {total_gb:.1f} GB total / {free_gb:.1f} GB free\")\n",
    "\n",
    "REQUIRED_GB = 20 if not TEST_MODE else 2\n",
    "\n",
    "if free_gb < REQUIRED_GB:\n",
    "    print(f\"\\n\" + \"!\" * 60)\n",
    "    print(f\"NOT ENOUGH SPACE! Need {REQUIRED_GB} GB free, have {free_gb:.1f} GB.\")\n",
    "    print(f\"Options:\")\n",
    "    print(f\"  1. Free up space in Google Drive\")\n",
    "    print(f\"  2. Buy Google One: https://one.google.com/about/plans\")\n",
    "    print(f\"     - 100 GB: $1.99/month\")\n",
    "    print(f\"     - 200 GB: $2.99/month\")\n",
    "    print(f\"  3. If you have Colab Pro, you may already have extra storage\")\n",
    "    print(f\"!\" * 60)\n",
    "    raise RuntimeError(f\"Not enough Google Drive space: {free_gb:.1f} GB free, need {REQUIRED_GB} GB\")\n",
    "else:\n",
    "    print(f\"\\nSpace OK: {free_gb:.1f} GB free >= {REQUIRED_GB} GB required\")\n",
    "\n",
    "# Create persistent directory structure on Google Drive\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/timoshka_data\"\n",
    "DRIVE_TTS_POS = os.path.join(DRIVE_BASE, \"tts_positive\")\n",
    "DRIVE_TTS_NEG = os.path.join(DRIVE_BASE, \"tts_negative\")\n",
    "DRIVE_VOICES = os.path.join(DRIVE_BASE, \"piper_voices\")\n",
    "DRIVE_TARGETS = os.path.join(DRIVE_BASE, \"voice_targets\")\n",
    "DRIVE_VC_POS = os.path.join(DRIVE_BASE, \"vc_positive\")\n",
    "DRIVE_VC_NEG = os.path.join(DRIVE_BASE, \"vc_negative\")\n",
    "DRIVE_OUTPUT = os.path.join(DRIVE_BASE, \"output\")\n",
    "\n",
    "for d in [DRIVE_BASE, DRIVE_TTS_POS, DRIVE_TTS_NEG, DRIVE_VOICES,\n",
    "          DRIVE_TARGETS, DRIVE_VC_POS, DRIVE_VC_NEG, DRIVE_OUTPUT]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Local working directories (ephemeral, fast I/O)\n",
    "LOCAL_BASE = \"/content/timoshka\"\n",
    "LOCAL_VC_POS = os.path.join(LOCAL_BASE, \"vc_positive\")\n",
    "LOCAL_VC_NEG = os.path.join(LOCAL_BASE, \"vc_negative\")\n",
    "\n",
    "for d in [LOCAL_BASE, LOCAL_VC_POS, LOCAL_VC_NEG]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\nDrive base: {DRIVE_BASE}\")\n",
    "print(f\"Local base: {LOCAL_BASE}\")\n",
    "\n",
    "# Report what's already on Drive\n",
    "import glob\n",
    "for label, path in [(\"TTS positive\", DRIVE_TTS_POS), (\"TTS negative\", DRIVE_TTS_NEG),\n",
    "                     (\"Voice targets\", DRIVE_TARGETS), (\"VC positive\", DRIVE_VC_POS),\n",
    "                     (\"VC negative\", DRIVE_VC_NEG)]:\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        count += len([f for f in files if f.endswith('.wav')])\n",
    "    if count > 0:\n",
    "        print(f\"  {label}: {count} WAV files (resumable!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repositories\n",
    "cd /content\n",
    "\n",
    "if [ ! -d \"openWakeWord\" ]; then\n",
    "    git clone https://github.com/dscripka/openWakeWord.git\n",
    "fi\n",
    "\n",
    "if [ ! -d \"piper-sample-generator\" ]; then\n",
    "    git clone https://github.com/rhasspy/piper-sample-generator.git\n",
    "fi\n",
    "\n",
    "echo \"Done cloning repos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\n# Install dependencies\npip install -q piper-phonemize -f https://github.com/rhasspy/piper-phonemize/releases/latest 2>/dev/null || true\npip install -q webrtcvad\npip install -q -e /content/openWakeWord\npip install -q -e /content/piper-sample-generator\npip install -q \"numpy<2\"\npip install -q coqui-tts\npip install -q mutagen torchinfo torchmetrics speechbrain\npip install -q audiomentations torch-audiomentations acoustics\npip install -q pronouncing deep-phonemizer\npip install -q onnx onnx2tf tf2onnx flatbuffers\n\n# Fix speechbrain/torchaudio compatibility (torchaudio 2.5+ removed list_audio_backends)\n# Applied unconditionally after all pip installs to ensure it's never overwritten\npython3 -c \"\nimport pathlib\nf = pathlib.Path('/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py')\nif not f.exists():\n    # Try python3.11 path\n    f = pathlib.Path('/usr/local/lib/python3.11/dist-packages/speechbrain/utils/torch_audio_backend.py')\nif f.exists():\n    code = f.read_text()\n    old = 'available_backends = torchaudio.list_audio_backends()'\n    new = 'available_backends = torchaudio.list_audio_backends() if hasattr(torchaudio, \\\"list_audio_backends\\\") else []'\n    if old in code:\n        code = code.replace(old, new)\n        f.write_text(code)\n        print('Patched speechbrain torchaudio compat')\n    else:\n        print('speechbrain already patched or compatible')\nelse:\n    print('WARNING: speechbrain torch_audio_backend.py not found')\n\"\n\necho \"\\nAll dependencies installed\"\necho \"IMPORTANT: If this is a fresh runtime, restart it now (Runtime -> Restart session)\"\necho \"Then re-run all cells from the top.\""
  },
  {
   "cell_type": "code",
   "source": "# Auto-restart runtime if numpy 2.x is still loaded in memory\n# (pip installed numpy<2 on disk, but Colab pre-loaded numpy 2.x)\nimport numpy as np\nimport os\n\n_RESTART_MARKER = '/content/.deps_installed'\n\nif int(np.__version__.split('.')[0]) >= 2:\n    if not os.path.exists(_RESTART_MARKER):\n        # First time: mark that deps are installed, then restart\n        open(_RESTART_MARKER, 'w').write('1')\n        print(f\"numpy {np.__version__} in memory, need <2. Restarting runtime...\")\n        print(\"After restart, click Runtime -> Run All (Ctrl+F9) to continue.\")\n        import IPython\n        IPython.Application.instance().kernel.do_shutdown(True)\n    else:\n        raise RuntimeError(\n            f\"numpy {np.__version__} still loaded after restart! \"\n            \"Try: Runtime -> Restart session, then Run All.\"\n        )\nelse:\n    print(f\"numpy {np.__version__} OK\")\n    # Mark deps as installed (for resume after disconnect)\n    open(_RESTART_MARKER, 'w').write('1')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Generate TTS samples (Piper)\n",
    "\n",
    "TTS samples are saved to Google Drive. If they already exist, this step is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\n# Download Russian Piper voices to Google Drive\ncd /content/drive/MyDrive/timoshka_data/piper_voices\n\nVOICES=(\n    \"ru/ru_RU/irina/medium/ru_RU-irina-medium\"\n    \"ru/ru_RU/ruslan/medium/ru_RU-ruslan-medium\"\n    \"ru/ru_RU/denis/medium/ru_RU-denis-medium\"\n    \"ru/ru_RU/dmitri/medium/ru_RU-dmitri-medium\"\n)\n\nBASE_URL=\"https://huggingface.co/rhasspy/piper-voices/resolve/main\"\n\n# Only download first N_VOICES voices (set by Python config cell)\ncount=0\nfor voice in \"${VOICES[@]}\"; do\n    name=$(basename $voice)\n    if [ ! -f \"${name}.onnx\" ]; then\n        echo \"Downloading ${name}...\"\n        wget -q -O \"${name}.onnx\" \"${BASE_URL}/${voice}.onnx?download=true\"\n        wget -q -O \"${name}.onnx.json\" \"${BASE_URL}/${voice}.onnx.json?download=true\"\n    else\n        echo \"${name} already on Drive\"\n    fi\n    count=$((count + 1))\ndone\n\necho \"\\nAll voices ready:\"\nls -la *.onnx"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport glob\nimport os\n\nvoices = sorted(glob.glob(os.path.join(DRIVE_VOICES, \"*.onnx\")))[:N_VOICES]\nprint(f\"Using {len(voices)} Piper voice(s)\")\n\nPOSITIVE_PHRASE = \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"  # тимошка\n\nfor voice_path in voices:\n    voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n    out_dir = os.path.join(DRIVE_TTS_POS, voice_name)\n    os.makedirs(out_dir, exist_ok=True)\n\n    existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n    if existing >= SAMPLES_PER_VOICE:\n        print(f\"  {voice_name}: {existing} samples on Drive, skipping\")\n        continue\n\n    print(f\"  Generating {SAMPLES_PER_VOICE} positive samples with {voice_name}...\")\n    subprocess.run([\n        \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n        POSITIVE_PHRASE,\n        \"--model\", voice_path,\n        \"--max-samples\", str(SAMPLES_PER_VOICE),\n        \"--output-dir\", out_dir,\n    ], check=True)\n    generated = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n    print(f\"    Generated: {generated}\")\n\ntotal = sum(\n    len(glob.glob(os.path.join(DRIVE_TTS_POS, d, \"*.wav\")))\n    for d in os.listdir(DRIVE_TTS_POS)\n    if os.path.isdir(os.path.join(DRIVE_TTS_POS, d))\n)\nprint(f\"\\nTotal positive TTS samples (on Drive): {total}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ALL_NEGATIVE_PHRASES = [\n    \"\\u0442\\u0438\\u043c\\u043e\\u0444\\u0435\\u0439\",      # тимофей\n    \"\\u043a\\u043e\\u0448\\u043a\\u0430\",                    # кошка\n    \"\\u043c\\u043e\\u0448\\u043a\\u0430\",                    # мошка\n    \"\\u0440\\u043e\\u043c\\u0430\\u0448\\u043a\\u0430\",      # ромашка\n    \"\\u043c\\u0430\\u0442\\u0440\\u0451\\u0448\\u043a\\u0430\", # матрёшка\n    \"\\u0433\\u0430\\u0440\\u043c\\u043e\\u0448\\u043a\\u0430\", # гармошка\n    \"\\u043a\\u0430\\u0440\\u0442\\u043e\\u0448\\u043a\\u0430\", # картошка\n    \"\\u043e\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",      # окрошка\n    \"\\u043c\\u0438\\u0448\\u043a\\u0430\",                    # мишка\n    \"\\u043c\\u044b\\u0448\\u043a\\u0430\",                    # мышка\n    \"\\u0442\\u0438\\u0448\\u043a\\u0430\",                    # тишка\n    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0430\",            # тимоша\n    \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u0435\\u043d\\u043a\\u043e\", # тимошенко\n    \"\\u043c\\u043e\\u0440\\u043e\\u0448\\u043a\\u0430\",      # морошка\n    \"\\u043a\\u0440\\u043e\\u0448\\u043a\\u0430\",            # крошка\n    \"\\u0434\\u043e\\u0440\\u043e\\u0436\\u043a\\u0430\",      # дорожка\n    \"\\u043b\\u043e\\u0436\\u043a\\u0430\",                    # ложка\n    \"\\u0442\\u0438\\u0448\\u0438\\u043d\\u0430\",            # тишина\n    \"\\u0442\\u0451\\u043c\\u0443\\u0448\\u043a\\u0430\",      # тёмушка\n]\n\nNEGATIVE_PHRASES = ALL_NEGATIVE_PHRASES[:NEG_PHRASES_COUNT]\nprint(f\"Using {len(NEGATIVE_PHRASES)} negative phrases, {NEG_SAMPLES_PER} samples each, {len(voices)} voice(s)\")\n\nfor phrase in NEGATIVE_PHRASES:\n    for voice_path in voices:\n        voice_name = os.path.basename(voice_path).replace(\".onnx\", \"\")\n        safe_phrase = phrase.replace(\"\\u0451\", \"\\u0435\")\n        out_dir = os.path.join(DRIVE_TTS_NEG, f\"{safe_phrase}_{voice_name}\")\n        os.makedirs(out_dir, exist_ok=True)\n\n        existing = len(glob.glob(os.path.join(out_dir, \"*.wav\")))\n        if existing >= NEG_SAMPLES_PER:\n            continue\n\n        subprocess.run([\n            \"python3\", \"/content/piper-sample-generator/generate_samples.py\",\n            phrase,\n            \"--model\", voice_path,\n            \"--max-samples\", str(NEG_SAMPLES_PER),\n            \"--output-dir\", out_dir,\n        ], check=True)\n\n    print(f\"  Done: {phrase}\")\n\ntotal_neg = 0\nfor root, dirs, files in os.walk(DRIVE_TTS_NEG):\n    total_neg += len([f for f in files if f.endswith(\".wav\")])\n\nprint(f\"\\nTotal negative TTS samples (on Drive): {total_neg}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few samples\n",
    "import IPython.display as ipd\n",
    "\n",
    "sample_files = glob.glob(os.path.join(DRIVE_TTS_POS, \"*/*.wav\"))[:3]\n",
    "for f in sample_files:\n",
    "    print(f\"Playing: {os.path.basename(os.path.dirname(f))}/{os.path.basename(f)}\")\n",
    "    ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Prepare target voices (Common Voice)\n",
    "\n",
    "Downloads Russian speech samples from Common Voice for voice conversion diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, io, random, subprocess, tarfile, requests\n",
    "\n",
    "existing_targets = len(glob.glob(os.path.join(DRIVE_TARGETS, \"*.wav\")))\n",
    "if existing_targets >= N_TARGET_VOICES:\n",
    "    print(f\"Already have {existing_targets} target voices on Drive, skipping download\")\n",
    "else:\n",
    "    HF_REPO = \"https://huggingface.co/datasets/fsicoli/common_voice_17_0/resolve/main\"\n",
    "\n",
    "    print(\"Downloading dev.tsv metadata...\")\n",
    "    tsv_url = f\"{HF_REPO}/transcript/ru/dev.tsv\"\n",
    "    tsv_resp = requests.get(tsv_url)\n",
    "    tsv_resp.raise_for_status()\n",
    "    print(f\"  Downloaded {len(tsv_resp.content) / 1024:.0f} KB\")\n",
    "\n",
    "    reader = csv.DictReader(io.StringIO(tsv_resp.text), delimiter=\"\\t\")\n",
    "    rows = list(reader)\n",
    "    print(f\"  {len(rows)} clips in dev split\")\n",
    "\n",
    "    speaker_clips = {}\n",
    "    for row in rows:\n",
    "        cid = row[\"client_id\"]\n",
    "        if cid not in speaker_clips:\n",
    "            speaker_clips[cid] = row[\"path\"]\n",
    "    print(f\"  {len(speaker_clips)} unique speakers\")\n",
    "\n",
    "    tar_url = f\"{HF_REPO}/audio/ru/dev/ru_dev_0.tar\"\n",
    "    tar_path = \"/content/timoshka/ru_dev_0.tar\"\n",
    "\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(f\"Downloading dev audio tar (~375 MB)...\")\n",
    "        with requests.get(tar_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0))\n",
    "            downloaded = 0\n",
    "            with open(tar_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8 * 1024 * 1024):\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total:\n",
    "                        print(f\"\\r  {downloaded / 1024**2:.0f} / {total / 1024**2:.0f} MB\", end=\"\", flush=True)\n",
    "        print(f\"\\n  Download complete\")\n",
    "    else:\n",
    "        print(f\"Dev tar already downloaded\")\n",
    "\n",
    "    speakers = list(speaker_clips.items())\n",
    "    random.seed(42)  # deterministic for reproducibility\n",
    "    random.shuffle(speakers)\n",
    "    wanted_clips = {}\n",
    "    for i, (cid, clip_path) in enumerate(speakers):\n",
    "        if len(wanted_clips) >= N_TARGET_VOICES:\n",
    "            break\n",
    "        wanted_clips[clip_path] = i\n",
    "\n",
    "    print(f\"Extracting {len(wanted_clips)} clips from tar...\")\n",
    "    saved = existing_targets\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        for member in tar:\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "            basename = os.path.basename(member.name)\n",
    "            if basename in wanted_clips:\n",
    "                try:\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        continue\n",
    "                    audio_bytes = f.read()\n",
    "                    tmp_mp3 = f\"/tmp/cv_clip_{saved}.mp3\"\n",
    "                    with open(tmp_mp3, \"wb\") as tmp:\n",
    "                        tmp.write(audio_bytes)\n",
    "\n",
    "                    out_path = os.path.join(DRIVE_TARGETS, f\"speaker_{saved:04d}.wav\")\n",
    "                    if os.path.exists(out_path):\n",
    "                        saved += 1\n",
    "                        os.remove(tmp_mp3)\n",
    "                        continue\n",
    "\n",
    "                    result = subprocess.run([\n",
    "                        \"ffmpeg\", \"-y\", \"-i\", tmp_mp3,\n",
    "                        \"-ar\", \"16000\", \"-ac\", \"1\", \"-f\", \"wav\", out_path\n",
    "                    ], capture_output=True, timeout=30)\n",
    "                    if result.returncode != 0:\n",
    "                        continue\n",
    "\n",
    "                    probe = subprocess.run([\n",
    "                        \"ffprobe\", \"-v\", \"error\",\n",
    "                        \"-show_entries\", \"format=duration\",\n",
    "                        \"-of\", \"csv=p=0\", out_path\n",
    "                    ], capture_output=True, text=True, timeout=10)\n",
    "                    duration = float(probe.stdout.strip())\n",
    "                    if duration < 3.0 or duration > 15.0:\n",
    "                        os.remove(out_path)\n",
    "                        continue\n",
    "\n",
    "                    saved += 1\n",
    "                    os.remove(tmp_mp3)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {basename}: {e}\")\n",
    "            if saved >= N_TARGET_VOICES:\n",
    "                break\n",
    "\n",
    "    print(f\"\\nSaved {saved} target voice files to Drive\")\n",
    "\n",
    "target_files = sorted(glob.glob(os.path.join(DRIVE_TARGETS, \"*.wav\")))\n",
    "print(f\"\\nTarget voice files on Drive: {len(target_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Voice Conversion (FreeVC24)\n",
    "\n",
    "**Resilient to disconnects:** VC results are organized by target voice on Google Drive.\n",
    "After completing each target voice, results are synced to Drive.\n",
    "On resume, completed targets are skipped automatically.\n",
    "\n",
    "Maximum data loss on disconnect: 1 target voice (~3-5 minutes of work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time, json, shutil\nfrom pathlib import Path\nfrom TTS.api import TTS\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Loading FreeVC24 model on {device}...\")\nvc_model = TTS(\"voice_conversion_models/multilingual/vctk/freevc24\").to(device)\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time, json, os, glob\nfrom pathlib import Path\n\ndef collect_source_files(source_dir):\n    \"\"\"Collect all WAV files from source directory.\"\"\"\n    files = []\n    for root, dirs, fnames in os.walk(source_dir):\n        for f in sorted(fnames):\n            if f.endswith(\".wav\"):\n                files.append(os.path.join(root, f))\n    files.sort()\n    return files\n\ndef make_output_name(src_path, source_dir, tgt_name):\n    \"\"\"Generate output filename from source and target.\"\"\"\n    src_name = Path(src_path).stem\n    parent_name = Path(src_path).parent.name\n    if parent_name != Path(source_dir).name:\n        prefix = f\"{parent_name}_{src_name}\"\n    else:\n        prefix = src_name\n    return f\"{prefix}_vc{tgt_name}.wav\"\n\ndef run_vc_by_target(source_dir, drive_output_dir, target_dir, label=\"\", max_sources=None):\n    \"\"\"Run voice conversion organized by target voice.\n    \n    For each target voice:\n    1. Check if target is already complete on Drive -> skip\n    2. Convert all sources with this target (on local disk for speed)\n    3. Copy results to Drive\n    4. Mark target as complete\n    \n    Maximum data loss on crash: 1 target's conversions.\n    \n    Args:\n        max_sources: If set, limit the number of source files to process.\n                     Useful in TEST_MODE to avoid processing cached full-mode data.\n    \"\"\"\n    source_files = collect_source_files(source_dir)\n    if max_sources:\n        source_files = source_files[:max_sources]\n    target_files = sorted(glob.glob(os.path.join(target_dir, \"*.wav\")))\n    \n    total_targets = len(target_files)\n    total_conversions = len(source_files) * total_targets\n    \n    # Status file on Drive tracks completed targets\n    status_path = os.path.join(drive_output_dir, \"_completion_status.json\")\n    if os.path.exists(status_path):\n        with open(status_path) as f:\n            completion = json.load(f)\n    else:\n        completion = {\"completed_targets\": [], \"total_files\": 0}\n    \n    completed_set = set(completion[\"completed_targets\"])\n    \n    print(f\"{label}Sources: {len(source_files)}, Targets: {total_targets} -> {total_conversions} conversions\")\n    print(f\"{label}Already completed targets: {len(completed_set)}/{total_targets}\")\n    \n    if len(completed_set) == total_targets:\n        total_files = completion.get(\"total_files\", 0)\n        print(f\"{label}ALL TARGETS COMPLETE! {total_files} files on Drive.\")\n        return total_files\n    \n    done_total = len(completed_set) * len(source_files)\n    t0 = time.time()\n    errors = 0\n    \n    for tgt_idx, tgt_path in enumerate(target_files):\n        tgt_name = Path(tgt_path).stem\n        \n        if tgt_name in completed_set:\n            continue\n        \n        # Create target subdirectory on Drive\n        tgt_drive_dir = os.path.join(drive_output_dir, tgt_name)\n        os.makedirs(tgt_drive_dir, exist_ok=True)\n        \n        # Check if this target is partially done on Drive\n        existing_on_drive = set(os.listdir(tgt_drive_dir))\n        \n        tgt_t0 = time.time()\n        converted_this_target = 0\n        skipped_this_target = 0\n        \n        for src_idx, src_path in enumerate(source_files):\n            out_name = make_output_name(src_path, source_dir, tgt_name)\n            \n            # Skip if already on Drive\n            if out_name in existing_on_drive:\n                skipped_this_target += 1\n                done_total += 1\n                continue\n            \n            # Convert to local disk first (fast), then copy to Drive\n            local_path = os.path.join(LOCAL_VC_POS if 'positive' in label.lower() else LOCAL_VC_NEG, out_name)\n            drive_path = os.path.join(tgt_drive_dir, out_name)\n            \n            try:\n                vc_model.voice_conversion_to_file(\n                    source_wav=src_path, target_wav=tgt_path, file_path=local_path,\n                )\n                # Copy to Drive immediately\n                shutil.copy2(local_path, drive_path)\n                os.remove(local_path)  # free local space\n                converted_this_target += 1\n                done_total += 1\n            except Exception as e:\n                errors += 1\n                done_total += 1\n                if errors <= 10:\n                    print(f\"{label}Error: {Path(src_path).stem} + {tgt_name}: {e}\")\n            \n            # Progress every 100 conversions\n            if (converted_this_target + skipped_this_target) % 100 == 0:\n                elapsed = time.time() - t0\n                rate = (done_total - len(completed_set) * len(source_files)) / max(elapsed, 1)\n                eta = (total_conversions - done_total) / max(rate, 0.01)\n                pct = 100 * done_total / total_conversions\n                bar_len = int(pct / 2)\n                bar = \"#\" * bar_len + \".\" * (50 - bar_len)\n                print(\n                    f\"{label}[{bar}] {done_total}/{total_conversions} ({pct:.1f}%) \"\n                    f\"| {rate:.1f}/s | ETA {eta/60:.0f}min | target {tgt_idx+1}/{total_targets} \"\n                    f\"| err={errors}\"\n                )\n        \n        # Mark target as complete\n        tgt_elapsed = time.time() - tgt_t0\n        completed_set.add(tgt_name)\n        completion[\"completed_targets\"] = list(completed_set)\n        completion[\"total_files\"] = done_total\n        with open(status_path, \"w\") as f:\n            json.dump(completion, f)\n        \n        print(\n            f\"{label}Target {tgt_name} DONE: {converted_this_target} converted, \"\n            f\"{skipped_this_target} skipped in {tgt_elapsed:.0f}s \"\n            f\"[{len(completed_set)}/{total_targets} targets complete]\"\n        )\n    \n    elapsed = time.time() - t0\n    print(f\"\\n{label}ALL DONE: {done_total} conversions, {errors} errors in {elapsed/60:.1f}min\")\n    return done_total\n\nprint(\"VC functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Positive voice conversion\nprint(\"=\" * 60)\nprint(\"POSITIVE SAMPLES: Voice Conversion\")\nprint(\"=\" * 60)\n\nn_pos_vc = run_vc_by_target(\n    source_dir=DRIVE_TTS_POS,\n    drive_output_dir=DRIVE_VC_POS,\n    target_dir=DRIVE_TARGETS,\n    label=\"[POS] \",\n    max_sources=SAMPLES_PER_VOICE if TEST_MODE else None,\n)\nprint(f\"\\nTotal positive VC samples: {n_pos_vc}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Negative voice conversion\nprint(\"=\" * 60)\nprint(\"NEGATIVE SAMPLES: Voice Conversion\")\nprint(\"=\" * 60)\n\nn_neg_vc = run_vc_by_target(\n    source_dir=DRIVE_TTS_NEG,\n    drive_output_dir=DRIVE_VC_NEG,\n    target_dir=DRIVE_TARGETS,\n    label=\"[NEG] \",\n    max_sources=NEG_SAMPLES_PER if TEST_MODE else None,\n)\nprint(f\"\\nTotal negative VC samples: {n_neg_vc}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample check (16kHz mono)\n",
    "import torchaudio\n",
    "\n",
    "def ensure_16k_mono(directory):\n",
    "    \"\"\"Check a random sample of WAVs in directory for 16kHz mono.\"\"\"\n",
    "    import random\n",
    "    all_wavs = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            if f.endswith('.wav'):\n",
    "                all_wavs.append(os.path.join(root, f))\n",
    "    \n",
    "    sample = random.sample(all_wavs, min(100, len(all_wavs)))\n",
    "    fixed = 0\n",
    "    for f in sample:\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(f)\n",
    "            changed = False\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "                changed = True\n",
    "            if sr != 16000:\n",
    "                waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "                changed = True\n",
    "            if changed:\n",
    "                torchaudio.save(f, waveform, 16000)\n",
    "                fixed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {f}: {e}\")\n",
    "    return fixed, len(sample)\n",
    "\n",
    "print(\"Spot-checking positive samples...\")\n",
    "fixed_pos, checked_pos = ensure_16k_mono(DRIVE_VC_POS)\n",
    "print(f\"  Checked {checked_pos}, fixed {fixed_pos}\")\n",
    "\n",
    "print(\"Spot-checking negative samples...\")\n",
    "fixed_neg, checked_neg = ensure_16k_mono(DRIVE_VC_NEG)\n",
    "print(f\"  Checked {checked_neg}, fixed {fixed_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to a few voice-converted samples\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "\n",
    "vc_dirs = [d for d in os.listdir(DRIVE_VC_POS) if os.path.isdir(os.path.join(DRIVE_VC_POS, d))]\n",
    "if vc_dirs:\n",
    "    sample_dir = os.path.join(DRIVE_VC_POS, random.choice(vc_dirs))\n",
    "    samples = glob.glob(os.path.join(sample_dir, \"*.wav\"))[:3]\n",
    "    for f in samples:\n",
    "        print(f\"Playing: {os.path.basename(f)}\")\n",
    "        ipd.display(ipd.Audio(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Download training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "if [ ! -f \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\" ]; then\n",
    "    echo \"Downloading ACAV100M features (~6 GB)...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "else\n",
    "    echo \"ACAV100M features already downloaded\"\n",
    "fi\n",
    "\n",
    "if [ ! -f \"validation_set_features.npy\" ]; then\n",
    "    echo \"Downloading validation set...\"\n",
    "    wget -q --show-progress \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "else\n",
    "    echo \"Validation set already downloaded\"\n",
    "fi\n",
    "\n",
    "echo \"\\nData files:\"\n",
    "ls -lh *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/timoshka\n",
    "\n",
    "if [ ! -d \"mit_rirs\" ]; then\n",
    "    echo \"Downloading MIT RIRs...\"\n",
    "    mkdir -p mit_rirs\n",
    "    wget -q --show-progress -O mit_rirs.zip \\\n",
    "        https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip\n",
    "    unzip -q mit_rirs.zip -d mit_rirs/ 2>/dev/null || true\n",
    "    rm -f mit_rirs.zip\n",
    "else\n",
    "    echo \"MIT RIRs already present\"\n",
    "fi\n",
    "\n",
    "if [ ! -d \"audioset_16k\" ]; then\n",
    "    echo \"Downloading AudioSet background noise subset...\"\n",
    "    mkdir -p audioset_16k\n",
    "    wget -q --show-progress -O audioset_16k.tar.gz \\\n",
    "        https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/audioset_16k_sample.tar.gz \\\n",
    "        2>/dev/null || echo \"Note: AudioSet subset not found.\"\n",
    "    if [ -f audioset_16k.tar.gz ]; then\n",
    "        tar -xzf audioset_16k.tar.gz -C audioset_16k/ 2>/dev/null || true\n",
    "        rm -f audioset_16k.tar.gz\n",
    "    fi\n",
    "fi\n",
    "\n",
    "mkdir -p fma\n",
    "echo \"Background data ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Train openWakeWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os, glob\n",
    "\n",
    "# Count all VC samples from Drive\n",
    "n_pos_vc = 0\n",
    "for root, dirs, files in os.walk(DRIVE_VC_POS):\n",
    "    n_pos_vc += len([f for f in files if f.endswith('.wav')])\n",
    "\n",
    "n_neg_vc = 0\n",
    "for root, dirs, files in os.walk(DRIVE_VC_NEG):\n",
    "    n_neg_vc += len([f for f in files if f.endswith('.wav')])\n",
    "\n",
    "n_pos_tts = 0\n",
    "for root, dirs, files in os.walk(DRIVE_TTS_POS):\n",
    "    n_pos_tts += len([f for f in files if f.endswith('.wav')])\n",
    "\n",
    "n_neg_tts = 0\n",
    "for root, dirs, files in os.walk(DRIVE_TTS_NEG):\n",
    "    n_neg_tts += len([f for f in files if f.endswith('.wav')])\n",
    "\n",
    "print(f\"VC positive:  {n_pos_vc}\")\n",
    "print(f\"VC negative:  {n_neg_vc}\")\n",
    "print(f\"TTS positive: {n_pos_tts}\")\n",
    "print(f\"TTS negative: {n_neg_tts}\")\n",
    "print(f\"Total positive: {n_pos_vc + n_pos_tts}\")\n",
    "print(f\"Total negative: {n_neg_vc + n_neg_tts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Merge all samples into flat directories for training\n",
    "# Using local disk for speed (symlinks don't work across Drive/local)\n",
    "ALL_POSITIVE_DIR = os.path.join(LOCAL_BASE, \"all_positive\")\n",
    "ALL_NEGATIVE_DIR = os.path.join(LOCAL_BASE, \"all_negative\")\n",
    "os.makedirs(ALL_POSITIVE_DIR, exist_ok=True)\n",
    "os.makedirs(ALL_NEGATIVE_DIR, exist_ok=True)\n",
    "\n",
    "def link_or_copy(src, dst):\n",
    "    if not os.path.exists(dst):\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "        except OSError:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "# Positive: VC from Drive + TTS from Drive\n",
    "print(\"Linking positive samples...\")\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(DRIVE_VC_POS):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            src = os.path.join(root, f)\n",
    "            tgt_name = os.path.basename(root)\n",
    "            link_or_copy(src, os.path.join(ALL_POSITIVE_DIR, f\"{tgt_name}_{f}\"))\n",
    "            count += 1\n",
    "for root, dirs, files in os.walk(DRIVE_TTS_POS):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            link_or_copy(src, os.path.join(ALL_POSITIVE_DIR, f\"tts_{parent}_{f}\"))\n",
    "            count += 1\n",
    "print(f\"  Linked {count} positive samples\")\n",
    "\n",
    "# Negative: VC from Drive + TTS from Drive\n",
    "print(\"Linking negative samples...\")\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(DRIVE_VC_NEG):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            src = os.path.join(root, f)\n",
    "            tgt_name = os.path.basename(root)\n",
    "            link_or_copy(src, os.path.join(ALL_NEGATIVE_DIR, f\"{tgt_name}_{f}\"))\n",
    "            count += 1\n",
    "for root, dirs, files in os.walk(DRIVE_TTS_NEG):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            src = os.path.join(root, f)\n",
    "            parent = os.path.basename(root)\n",
    "            link_or_copy(src, os.path.join(ALL_NEGATIVE_DIR, f\"tts_{parent}_{f}\"))\n",
    "            count += 1\n",
    "print(f\"  Linked {count} negative samples\")\n",
    "\n",
    "total_pos = len(os.listdir(ALL_POSITIVE_DIR))\n",
    "total_neg = len(os.listdir(ALL_NEGATIVE_DIR))\n",
    "print(f\"\\nAll positive: {total_pos}\")\n",
    "print(f\"All negative: {total_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "TRAINING_DIR = os.path.join(LOCAL_BASE, \"training\")\n",
    "os.makedirs(TRAINING_DIR, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"timoshka\",\n",
    "    \"target_phrase\": [\"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\"],\n",
    "    \"custom_negative_phrases\": [],\n",
    "    \"n_samples\": 0,\n",
    "    \"n_samples_val\": 0,\n",
    "    \"augmentation_rounds\": 1,\n",
    "    \"augmentation_batch_size\": 16,\n",
    "    \"piper_sample_generator_path\": \"/content/piper-sample-generator\",\n",
    "    \"output_dir\": TRAINING_DIR,\n",
    "    \"rir_paths\": [os.path.join(LOCAL_BASE, \"mit_rirs\")],\n",
    "    \"background_paths\": [\n",
    "        os.path.join(LOCAL_BASE, \"audioset_16k\"),\n",
    "        os.path.join(LOCAL_BASE, \"fma\"),\n",
    "    ],\n",
    "    \"background_paths_duplication_rate\": [1],\n",
    "    \"feature_data_files\": {\n",
    "        \"ACAV100M_sample\": os.path.join(\n",
    "            LOCAL_BASE, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
    "        ),\n",
    "    },\n",
    "    \"false_positive_validation_data_path\": os.path.join(\n",
    "        LOCAL_BASE, \"validation_set_features.npy\"\n",
    "    ),\n",
    "    \"batch_n_per_class\": {\n",
    "        \"ACAV100M_sample\": 1024,\n",
    "        \"adversarial_negative\": 50,\n",
    "        \"positive\": 50,\n",
    "    },\n",
    "    \"model_type\": \"dnn\",\n",
    "    \"layer_size\": 32,\n",
    "    \"steps\": TRAINING_STEPS,\n",
    "    \"max_negative_weight\": 1500,\n",
    "    \"target_false_positives_per_hour\": 0.2,\n",
    "    \"target_accuracy\": 0.7,\n",
    "    \"target_recall\": 0.5,\n",
    "}\n",
    "\n",
    "config_path = os.path.join(LOCAL_BASE, \"timoshka_config.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"Config saved ({TRAINING_STEPS} steps)\")\n",
    "print(open(config_path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory structure expected by openWakeWord train.py\n",
    "phrase_dir = os.path.join(TRAINING_DIR, \"\\u0442\\u0438\\u043c\\u043e\\u0448\\u043a\\u0430\")\n",
    "pos_link = os.path.join(phrase_dir, \"positive\")\n",
    "neg_link = os.path.join(phrase_dir, \"negative\")\n",
    "\n",
    "os.makedirs(phrase_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(pos_link):\n",
    "    os.remove(pos_link)\n",
    "os.symlink(ALL_POSITIVE_DIR, pos_link)\n",
    "\n",
    "if os.path.exists(neg_link):\n",
    "    os.remove(neg_link)\n",
    "os.symlink(ALL_NEGATIVE_DIR, neg_link)\n",
    "\n",
    "print(f\"Positive -> {ALL_POSITIVE_DIR} ({len(os.listdir(pos_link))} files)\")\n",
    "print(f\"Negative -> {ALL_NEGATIVE_DIR} ({len(os.listdir(neg_link))} files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\n# Apply speechbrain patch before running train.py (in case pip overwrote it)\ndef patch_speechbrain():\n    \"\"\"Fix speechbrain/torchaudio compatibility inline.\"\"\"\n    import pathlib\n    for pyver in [\"3.12\", \"3.11\", \"3.10\"]:\n        f = pathlib.Path(f'/usr/local/lib/python{pyver}/dist-packages/speechbrain/utils/torch_audio_backend.py')\n        if f.exists():\n            code = f.read_text()\n            old = 'available_backends = torchaudio.list_audio_backends()'\n            new = 'available_backends = torchaudio.list_audio_backends() if hasattr(torchaudio, \"list_audio_backends\") else []'\n            if old in code:\n                code = code.replace(old, new)\n                f.write_text(code)\n                print(f\"Patched {f}\")\n            return\n    print(\"WARNING: speechbrain torch_audio_backend.py not found\")\n\npatch_speechbrain()\n\nprint(\"Starting augmentation...\")\nresult = subprocess.run(\n    [sys.executable, \"openWakeWord/openwakeword/train.py\",\n     \"--training_config\", \"/content/timoshka/timoshka_config.yaml\",\n     \"--augment_clips\", \"--overwrite\"],\n    cwd=\"/content\",\n    capture_output=True, text=True,\n)\nprint(result.stdout[-3000:] if len(result.stdout) > 3000 else result.stdout)\nif result.returncode != 0:\n    print(f\"\\nSTDERR:\\n{result.stderr[-3000:] if len(result.stderr) > 3000 else result.stderr}\")\n    print(f\"\\nAugmentation failed with exit code {result.returncode}\")\nelse:\n    print(\"Augmentation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\npatch_speechbrain()  # ensure patch is applied\n\nprint(\"Starting training...\")\nresult = subprocess.run(\n    [sys.executable, \"openWakeWord/openwakeword/train.py\",\n     \"--training_config\", \"/content/timoshka/timoshka_config.yaml\",\n     \"--train_model\"],\n    cwd=\"/content\",\n    capture_output=True, text=True,\n)\nprint(result.stdout[-3000:] if len(result.stdout) > 3000 else result.stdout)\nif result.returncode != 0:\n    print(f\"\\nSTDERR:\\n{result.stderr[-3000:] if len(result.stderr) > 3000 else result.stderr}\")\n    print(f\"\\nTraining failed with exit code {result.returncode}\")\nelse:\n    print(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\n\npatch_speechbrain()  # ensure patch is applied\n\nprint(\"Converting to TFLite...\")\nresult = subprocess.run(\n    [sys.executable, \"openWakeWord/openwakeword/train.py\",\n     \"--training_config\", \"/content/timoshka/timoshka_config.yaml\",\n     \"--convert_to_tflite\"],\n    cwd=\"/content\",\n    capture_output=True, text=True,\n)\nprint(result.stdout[-3000:] if len(result.stdout) > 3000 else result.stdout)\nif result.returncode != 0:\n    print(f\"\\nSTDERR:\\n{result.stderr[-3000:] if len(result.stderr) > 3000 else result.stderr}\")\n    print(f\"\\nConversion failed with exit code {result.returncode}\")\nelse:\n    print(\"Conversion complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "TRAINING_DIR = os.path.join(LOCAL_BASE, \"training\")\n",
    "tflite_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.tflite\"), recursive=True)\n",
    "onnx_files = glob.glob(os.path.join(TRAINING_DIR, \"**/*.onnx\"), recursive=True)\n",
    "\n",
    "print(\"Generated model files:\")\n",
    "for f in tflite_files + onnx_files:\n",
    "    size = os.path.getsize(f)\n",
    "    print(f\"  {f} ({size/1024:.1f} KB)\")\n",
    "\n",
    "if tflite_files:\n",
    "    # Save to Drive for persistence\n",
    "    drive_model = os.path.join(DRIVE_OUTPUT, \"timoshka.tflite\")\n",
    "    shutil.copy2(tflite_files[0], drive_model)\n",
    "    print(f\"\\nModel saved to Drive: {drive_model}\")\n",
    "    print(f\"Size: {os.path.getsize(drive_model)/1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"\\nERROR: No .tflite file found. Check training logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from openwakeword.model import Model\n",
    "\n",
    "model_path = os.path.join(DRIVE_OUTPUT, \"timoshka.tflite\")\n",
    "oww_model = Model(wakeword_models=[model_path])\n",
    "model_name = list(oww_model.models.keys())[0]\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "\n",
    "def test_wav(wav_path, model, name):\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "    if sr != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    audio = (waveform.squeeze().numpy() * 32767).astype(np.int16)\n",
    "    model.reset()\n",
    "    chunk_size = 1280\n",
    "    max_score = 0.0\n",
    "    for i in range(0, len(audio) - chunk_size, chunk_size):\n",
    "        chunk = audio[i:i+chunk_size]\n",
    "        prediction = model.predict(chunk)\n",
    "        score = prediction[name]\n",
    "        max_score = max(max_score, score)\n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"POSITIVE SAMPLES (should trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "pos_files = []\n",
    "for root, dirs, files in os.walk(DRIVE_VC_POS):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            pos_files.append(os.path.join(root, f))\n",
    "test_pos = random.sample(pos_files, min(50, len(pos_files)))\n",
    "\n",
    "pos_scores = []\n",
    "for f in test_pos:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    pos_scores.append(score)\n",
    "\n",
    "triggered = sum(1 for s in pos_scores if s >= 0.5)\n",
    "print(f\"Tested: {len(test_pos)}\")\n",
    "print(f\"Triggered (>0.5): {triggered}/{len(test_pos)} ({100*triggered/len(test_pos):.0f}%)\")\n",
    "print(f\"Mean: {np.mean(pos_scores):.3f}, Min: {np.min(pos_scores):.3f}, Max: {np.max(pos_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"NEGATIVE SAMPLES (should NOT trigger)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "neg_files = []\n",
    "for root, dirs, files in os.walk(DRIVE_VC_NEG):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            neg_files.append(os.path.join(root, f))\n",
    "test_neg = random.sample(neg_files, min(100, len(neg_files)))\n",
    "\n",
    "neg_scores = []\n",
    "false_positives = []\n",
    "for f in test_neg:\n",
    "    score = test_wav(f, oww_model, model_name)\n",
    "    neg_scores.append(score)\n",
    "    if score >= 0.5:\n",
    "        false_positives.append((os.path.basename(f), score))\n",
    "\n",
    "print(f\"Tested: {len(test_neg)}\")\n",
    "print(f\"False positives (>0.5): {len(false_positives)}/{len(test_neg)} ({100*len(false_positives)/len(test_neg):.1f}%)\")\n",
    "print(f\"Mean: {np.mean(neg_scores):.3f}, Min: {np.min(neg_scores):.3f}, Max: {np.max(neg_scores):.3f}\")\n",
    "\n",
    "if false_positives:\n",
    "    print(\"\\nWorst false positives:\")\n",
    "    for fname, score in sorted(false_positives, key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {fname}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tp_rate = sum(1 for s in pos_scores if s >= 0.5) / len(pos_scores) * 100\n",
    "fp_rate = len(false_positives) / len(test_neg) * 100\n",
    "\n",
    "print(f\"True positive rate:  {tp_rate:.0f}%\")\n",
    "print(f\"False positive rate: {fp_rate:.1f}%\")\n",
    "print()\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"TEST MODE results (not production quality).\")\n",
    "    print(\"Set TEST_MODE = False and re-run for full training.\")\n",
    "elif tp_rate >= 70 and fp_rate < 5:\n",
    "    print(\"Model looks good! Ready for deployment.\")\n",
    "elif tp_rate >= 50:\n",
    "    print(\"Model is acceptable. Consider more training or data.\")\n",
    "else:\n",
    "    print(\"Model needs improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "model_path = os.path.join(DRIVE_OUTPUT, \"timoshka.tflite\")\n",
    "if os.path.exists(model_path):\n",
    "    files.download(model_path)\n",
    "    print(f\"Downloaded: timoshka.tflite ({os.path.getsize(model_path)/1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"Model file not found. Check training logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Home Assistant\n",
    "\n",
    "After downloading `timoshka.tflite`:\n",
    "\n",
    "```bash\n",
    "# 1. Copy model to server\n",
    "scp timoshka.tflite v@plex.local:/home/v/home-assistant/openwakeword-data/timoshka.tflite\n",
    "\n",
    "# 2. Restart the openwakeword container\n",
    "ssh v@plex.local 'cd /home/v/home-assistant && docker compose restart openwakeword'\n",
    "\n",
    "# 3. In Home Assistant:\n",
    "#    Settings -> Voice assistants -> your assistant -> Wake word -> select \"timoshka\"\n",
    "#\n",
    "# No changes needed to ESPHome/Atom Echo — wake word is processed server-side.\n",
    "```"
   ]
  }
 ]
}